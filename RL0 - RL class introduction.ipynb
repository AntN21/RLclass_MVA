{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/reinforcement-learning/\">https://supaerodatascience.github.io/reinforcement-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Class 0: Reinforcement Learning class introduction</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword <a class=\"tocSkip\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How this course works (pedagogically):\n",
    "- a series of notebooks\n",
    "- no slides\n",
    "- short exercices along the way\n",
    "- a bit of live coding\n",
    "    \n",
    "What you should expect:\n",
    "- some plain words notions,\n",
    "- but avoidance of over-simplification,\n",
    "- and also a fair amount of (hopefully painless) rigorous notations and abstract concepts.\n",
    "- Also most things will be fully written down to increase your autonomy in replaying the notebook.\n",
    "\n",
    "Color code:\n",
    "<div class=\"alert alert-success\">Key results in green boxes</div>\n",
    "<div class=\"alert alert-warning\">Exercices in yellow boxes</div>\n",
    "<div class=\"alert alert-danger\">Solutions in red boxes</div>\n",
    "\n",
    "And a first yellow box:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic algebra.\n",
    "- Random variables, probability distributions.\n",
    "- Gradient descent.\n",
    "    \n",
    "**Useful but not compulsory:**\n",
    "- Random processes, Markov chains.\n",
    "- Notion of contraction mapping.\n",
    "- Dynamic Programming\n",
    "- Stochastic Gradient Descent.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1><span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Class-goals\" data-toc-modified-id=\"Class-goals-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Class goals</a></span></li><li><span><a href=\"#Ruining-the-suspense-with-a-general-abstract-definition-(5-minutes)\" data-toc-modified-id=\"Ruining-the-suspense-with-a-general-abstract-definition-(5-minutes)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Ruining the suspense with a general abstract definition (5 minutes)</a></span></li><li><span><a href=\"#RL-within-Machine-Learning-(5-minutes)\" data-toc-modified-id=\"RL-within-Machine-Learning-(5-minutes)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>RL within Machine Learning (5 minutes)</a></span></li><li><span><a href=\"#From-plain-words-to-first-variables-(5-minutes)\" data-toc-modified-id=\"From-plain-words-to-first-variables-(5-minutes)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>From plain words to first variables (5 minutes)</a></span></li><li><span><a href=\"#Vocabulary\" data-toc-modified-id=\"Vocabulary-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Vocabulary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- acquire the fundamental building blocks of RL:\n",
    "    - plain word notions\n",
    "    - MDPs, policies, optimality equations, etc.\n",
    "    - common notations\n",
    "    - key algorithms\n",
    "    - common misconceptions\n",
    "- key challenges in RL and their connection to future lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ruining the suspense with a general abstract definition (5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Reinforcement Learning about?\n",
    "\n",
    "It is about learning to control dynamic systems.\n",
    "<img src=\"img/dynamic.png\" style=\"width: 400px;\"></img>\n",
    "Dynamic systems? **dynamic** evolution of $s$ and $o$ under $\\pi$ over a certain time horizon.\n",
    "\n",
    "Our object of study:<br>\n",
    "We want to find a control policy $\\pi$ (with $u = \\pi(o)$) such that the system $\\Sigma$ behaves as we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of RL problems <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td><img src=\"img/spiral.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Exiting a spiral</td>\n",
    "  <td><img src=\"img/tests.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Dynamic treatment regimes for HIV patients</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/pend.png\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Cart-pole balancing</td>\n",
    "  <td><img src=\"img/waiting.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Queueing problems</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/market.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Portfolio management</td>\n",
    "  <td><img src=\"img/dam.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Hydroelectric production</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "But also:\n",
    "- Elevator scheduling\n",
    "- Bicyle riding\n",
    "- Ship steering\n",
    "- Bioreactor control\n",
    "- Aerobatics helicopter control\n",
    "- Airport departures scheduling\n",
    "- Ecosystem regulation and preservation\n",
    "- Robocup soccer\n",
    "- Video game playing (Atari, Starcraft...)\n",
    "- Game of Go\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, learning to play a board game, learning to juggle, learning to take good strategic decisions, learning to drive... all fall into the same category of **control problems** and Reinforcement Learning studies the process of **elaborating a good control strategy through interaction samples**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "Reinforcement Learning is about learning an optimal sequential behavior in a given environment.\n",
    "</div>\n",
    "\n",
    "Let's break this down.\n",
    "- sequential behavior in a given environment  \n",
    "$\\rightarrow$ discrete time steps, sequence of actions\n",
    "- optimal  \n",
    "$\\rightarrow$ a reward signal informs us of the quality of the last action\n",
    "- learning  \n",
    "$\\rightarrow$ no known model a priori, just interaction samples, behavior adaptation.\n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"width: 400px;\"></img></center>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Keywords:**\n",
    "- system to control / environment\n",
    "- control policy\n",
    "- optimality\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Warm-up poll:** \n",
    "How do you do today?  \n",
    "[https://linkto.run/p/BOOR15YA](https://linkto.run/p/BOOR15YA)\n",
    "- Great, I'm learning RL!\n",
    "- Great, but I'm scared the RL unicorn will turn into a difficult to tame rhino.\n",
    "- Great, bring the math on (as long as you do it step by step).\n",
    "- Why do you ask the question if the only answer is \"Great\"?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standing on the shoulders of giants**\n",
    "\n",
    "> The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence. (Sutton & Barto, 2018, [Reinforcement Learning: an Introduction](http://incompleteideas.net/book/the-book-2nd.html))\n",
    "\n",
    "Caveat: this is a definition of *learning*, not specifically of *reinforcement learning* (although it applies to RL), so it is worth giving some context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL within Machine Learning (5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have had classes on Machine Learning before. There are three strongly distinct categories of problems in ML:\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "- Reinforcement Learning\n",
    "\n",
    "Let's try to answer the following questions for each category.\n",
    "- What's the abstract problem we are trying to solve?\n",
    "- What's the data provided to the algorithms?\n",
    "- Give examples of algorithms in SL/UL/RL.  \n",
    "\n",
    "<center>\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "    <td> <b>Question</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Supervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Unsupervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Reinforcement</b> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $f(x)=y$ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $x\\in X$ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\pi(s)=a$ </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target (rephrased) </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Predict outputs given inputs</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Discover structure in data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Find an optimal behavior </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{\\left(x,y\\right)\\right\\}$ supervisor's labels </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{x\\right\\}$ unlabelled data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $\\left\\{\\left(s,a,r,s'\\right)\\right\\}$ experience samples </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Output </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Classifier or regressor</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Clusters or dimension reduction </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Policies, value functions </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Key algorithms </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Neural networks, SVMs, etc.</td>\n",
    "    <td style=\"border-left: 1px solid black\"> k-means, PCA, etc. </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Q-learning, Policy Gradients, etc. </td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "This table helps distinguish the different natures of the problems tackled. The RL problem is about finding the optimal policy for a given environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this different from Supervised Learning?\n",
    "- no correct $(s,a)$ example, rather $(s,a,r,s')$ samples\n",
    "- Delayed rewards, credit assignment, trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll:** Pick the true statement(s).  \n",
    "[https://linkto.run/p/3OG3IJO3](https://linkto.run/p/3OG3IJO3)\n",
    "- Sorting new emails as spam (or not) given a million labelled emails is a reinforcement learning task.\n",
    "- Deciding what move to play at chess, based on thousands of previous games is a reinforcement learning task.\n",
    "- Incrementally improving the accuracy of a radar detection software from online collected data is a reinforcement learning task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspirations for RL:\n",
    "- Control theory and Stochastic processes for the **modeling** part\n",
    "- Statistics, Optimization and Cognitive Psychology for the **learning** part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From plain words to first variables (5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A medical prescription example <a class=\"tocSkip\">\n",
    "\n",
    "<img src=\"img/patient-doctor.png\" style=\"height: 200px;\">\n",
    "    \n",
    "A patient walks into a clinic with her medical file (medical history, x-rays, blood work, etc.). You, as her doctor, need to write a prescription. Let us use this example to formalize the process of deciding what to write on the prescription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Patient variables <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<img src=\"img/patient_file.png\" style=\"height: 100px;\"> </img> <br>\n",
    "Patient state now: $S_0$  <br>\n",
    "Future states: $S_t$\n",
    "</center>\n",
    "\n",
    "The medical file of the patient allows us to define a number of variables that characterize the patient now. We will write $S_0$ the vector of these variables. Future measurements will be noted $S_t$.\n",
    "\n",
    "$S_t$ is a random vector, taking different values in a *patient description space* $S$ at different time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prescription <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<img src=\"img/prescription.png\" style=\"height: 100px;\"> </img> <br>\n",
    "Prescription: $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$\n",
    "</center>\n",
    "\n",
    "The prescription is a series of recommendations we give to the patient over the course of treatment. It is thus a sequence $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$ of variables $A_t$.\n",
    "\n",
    "These treatments $A_t$ are random variables too, taking their value in some space $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient evolution <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"img/patient_evolution.png\" style=\"height: 100px;\"> </img> <br>\n",
    "    $\\mathbb{P}(S_t)$?\n",
    "</center>\n",
    "\n",
    "The patient evolves over time steps. Her evolution follows a certain probability distribution $\\mathbb{P}(S_t)$ over descriptive states.\n",
    "\n",
    "So $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ defines a *random process* that describes the patient's evolution under the influence of past $S_t$ and $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physician's goal <a class=\"tocSkip\">\n",
    "\n",
    "<img src=\"img/patient_happy.png\" style=\"height: 100px;\"> </img> <br>\n",
    "\n",
    "$$J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)?$$\n",
    "\n",
    "The physician's goal is to bring the patient from an unhealthy state $S_0$ to a healthy situation.  \n",
    "\n",
    "This goal is not only defined by a final state of the patient but by the full trajectory followed by the variables $S_t$ and $A_t$. For example, prescribing a drug that damages the patient's liver, or letting the patient experience too much pain over the course of treatment is discouraged.\n",
    "\n",
    "We define a criterion $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$ that allows to quantify how good a trajectory in the joint $S\\times A$ space is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up <a class=\"tocSkip\">\n",
    "\n",
    "- Patient state $S_t$  (random variable)\n",
    "- Physician instruction $A_t$ (random variable)\n",
    "- Prescription $\\left( A_t \\right)_{t\\in\\mathbb{N}}$   \n",
    "- Patient's evolution $\\mathbb{P}(S_t)$  \n",
    "- Patient's trajectory $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ random process\n",
    "- Value of a trajectory $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$  \n",
    "\n",
    "It seems reasonable that the physician's recommendation $\\mathbb{P}(A_t)$ at step $t$ be dependent on previously observed states $\\left(S_0, \\ldots, S_t\\right)$ and recommended treatments $\\left(A_0, \\ldots, A_{t-1}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common misconception <a class=\"tocSkip\">\n",
    "\n",
    "You will often see the following type of drawing, along with a sentence like \"RL is concerned with the problem on an agent performing actions to control an environment\". \n",
    "\n",
    "<img src=\"img/misconception.png\" style=\"height: 300px;\"></img>\n",
    "\n",
    "Although this sentence is not false *per se*, it conveys an important misconception that may be grounded in too simple anthropomorphic analogies. One often talks about the *state of the agent* or the *state of the environment*. The distinction here is confusing at best: there is no separation between agent and environment. A better vocabulary is to talk about a *system to control*, that is described through its observed *state*. This system is controlled by the application of actions issued from a *policy* or *control law*. The process of *learning* this policy is what RL is concerned with.\n",
    "\n",
    "Although less shiny, the drawing below may be less misleading.\n",
    "\n",
    "<img src=\"img/dynamic.png\" style=\"height: 300px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "- System to control\n",
    "- State $S_t$ at time step $t$\n",
    "- Action $A_t$ at time step $t$\n",
    "- Observation $O_t$ at time step $t$\n",
    "- System dynamics $\\mathbb{P}(S_t)$\n",
    "- Trajectory $(S_t, A_t)_{t\\in \\mathbb{N}}$\n",
    "- Value of a trajectory $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$ \n",
    "- Goal of RL: decide the probability $\\mathbb{P}(A_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three key notions <a class=\"tocSkip\">\n",
    "\n",
    "Understanding RL is a three-stage rocket, answering the questions:  \n",
    "1. What is the system to control?  \n",
    "2. What is an optimal strategy?  \n",
    "3. How do we learn such a strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (no poll, 1 minute):**  \n",
    "Suppose that, instead of treating a patient, we want to learn to swing the pole up in the cart-pole example.  \n",
    "What are the state description variables?  \n",
    "What are the action variables?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pend.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "State: cart position and velocity $x, \\dot{x}$, pole angle and velocity $\\theta, \\dot{\\theta}$.\n",
    "    \n",
    "Action: force $F$ applied on the cart.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "186px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "416.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

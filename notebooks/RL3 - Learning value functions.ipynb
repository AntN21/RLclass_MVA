{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030eda65-9fd3-4657-9e96-2e577b1098ff",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817d749-b572-4f25-b203-0c7e1a2ae3e8",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 3: Learning value functions</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6f01d-08c1-4df1-b0ee-6413e211e383",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**  \n",
    "By the end of this chapter, you should be able to:\n",
    "- explain approximate value iteration and recall its (non-)convergence properties\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85133920-4d7b-4586-b217-aa5831fd4bd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Approximate dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb7319-d217-404d-bc38-7cf812d89e19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's take a step back on the contents of the previous chapter.\n",
    "\n",
    "With the Bellman equation, we have a way to **characterize** $Q^*$. This characterization directly translates to the **Value Iteration** algorithm. In turn, once we know $Q^*$, we can deduce $\\pi^*$.\n",
    "\n",
    "That's all very nice, but is it applicable in practice, on real world examples? In particular, how does the computation of $Q^*$ scale with large state and action spaces?\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "Do you recall the time complexity of a single iteration of value iteration in terms of $|S|$ and $|A|$?  \n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$O(S^2 A)$\n",
    "</details>\n",
    "\n",
    "The curse of dimensionality makes the number of states and actions scale exponentially with the dimension of the state and action spaces. So exact computation of $Q^*$ quickly becomes intractable, as building $Q_{n+1} = T^*Q_n$ from $Q_n$ requires $|S|$ operations in every state-action pair (hence the complexity in the exercise above).\n",
    "\n",
    "Instead, one can try to *approximate* the resolution of $T^* Q_n$ at each step of Value Iteration. This yields the Approximate Value Iteration algorithm.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Approximate Value Iteration** is the algorithm that computes the sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$, where $\\mathcal{A}$ is an approximation procedure.\n",
    "</div>\n",
    "\n",
    "Note in particular that when dealing with parametric functions $Q_\\theta$, finding a minimizer of the loss\n",
    "$L_n(\\theta) = \\| Q_\\theta - T^* Q_n \\|$\n",
    "is such an approximation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b248c-b12d-4f79-9485-2bba1e441522",
   "metadata": {},
   "source": [
    "Let us suppose that $\\mathcal{A}$ is not a bad approximation procedure and that its approximation error is uniformly bounded, that is, \n",
    "$$\\forall f \\in \\mathbb{R}^{SA}, \\ \\| f-\\mathcal{A}f \\|_\\infty \\leq \\epsilon.$$\n",
    "\n",
    "The first important result is that Approximate Value Iteration **does not converge**. However, one can prove that $Q_n$ reaches a neighborhood of $Q^*$. Specifically, there exists $N$ such that for all $n\\geq N$,\n",
    "$$\\| Q^* - Q_n \\|_\\infty \\leq \\frac{\\epsilon}{1-\\gamma}.$$\n",
    "\n",
    "More importantly, let $\\pi_n$ be the greedy policy with respect to $Q_n$, then:\n",
    "$$\\|Q^*-Q^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma}{1-\\gamma} \\|Q^*-Q_n\\|_\\infty.$$\n",
    "\n",
    "And consequently, for such $n\\geq N$,\n",
    "$$\\|Q^*-Q^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}.$$\n",
    "\n",
    "So,\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Approximate Value Iteration does not necessarily converge but reaches policies whose values are close to optimal.\n",
    "</div>\n",
    "\n",
    "These results are proven in the **[Neuro-dynamic programming](http://athenasc.com/ndpbook.html)** book by D. P. Bertsekas and J. Tsitsiklis (1996).\n",
    "\n",
    "Most supervised learning algorithms minimize a loss that is expressed as a weighted $L_2$ norm. Thus, they don't explicitly provide guarantees in $L_\\infty$ norm. R. Munos provided **[error bounds for approximate value iteration](https://www.aaai.org/Papers/AAAI/2005/AAAI05-159.pdf)** in the general case of weighted $L_p$ norms. Those bounds are similar to the one in $L_\\infty$ norm, thus justifying the use of supervised learning techniques (such as neural networks or random forests for instance) in Approximate Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f6211-6628-487c-a834-15bc6d612bd8",
   "metadata": {},
   "source": [
    "**Approximate value iteration generalizes value iteration**\n",
    "\n",
    "Let us stress this out: approximate value iteration fully generalizes value iteration.  \n",
    "The Bellman optimality equation defines $Q^*$ as the solution to $Q=T^* Q$. This solution belongs to the space $\\mathbb{R}^{SA}$ of functions from $S\\times A$ to $\\mathbb{R}$. This space of functions is a vector space whose dimension is the cardinality of $S\\times A$. Hence, beyond the case of finite state and action spaces, no finite basis of functions can span $\\mathbb{R}^{SA}$ and searching for a solution to $Q=T^* Q$ is searching inside an infinite dimensional space.  \n",
    "\n",
    "When $\\mathcal{A}$ turns out to be exact (that is, its approximation error $\\epsilon$ is null), then AVI boils down to VI.  \n",
    "This can happen when $S$ and $A$ are discrete and the functions $\\mathbb{R}^{SA}$ are represented exactly as vectors in $\\mathbb{R}^{|S||A|}$. But this can happen also in continuous state or action spaces with very specific hypotheses, where the optimality equation admits a closed-form solution. An example of this where the value function is a polynomial function of a continuous state variable is developped in these [two](https://papers.nips.cc/paper_files/paper/2000/hash/09b15d48a1514d8209b192a8b8f34e48-Abstract.html) [papers](https://ieeexplore.ieee.org/abstract/document/5364653).\n",
    "\n",
    "On the other hand, when the state-action space is continuous, or just too large to be enumerated, AVI trades exact construction of the $Q_{+1} = T^*Q_n$ sequence for scalability, by constructing an approximate sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$ that still provides good policies if $\\mathcal{A}$ is a good approximation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa61021-05ab-4a96-8949-20e1cf501f0f",
   "metadata": {},
   "source": [
    "**And approximate (modified) policy iteration?**\n",
    "\n",
    "Approximate Policy Iteration consists in solving the evaluation equation up to a certain precision $\\epsilon$ and then taking a greedy improvement step.\n",
    "\n",
    "We write $V_n$ the approximation of $V^{\\pi_n}$ at the end of an evaluation phase in Policy Iteration and suppose that the approximation error is uniformly bounded:\n",
    "$$\\| V^{\\pi_n} - V_n \\|_\\infty \\leq \\epsilon.$$\n",
    "\n",
    "Then it is known that, even though the sequence of greedy policies $\\pi_n$ does not converge, it oscillates among a set of policies such that:\n",
    "$$\\|V^*-V^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}.$$\n",
    "\n",
    "This result is also proven in the **[Neuro-dynamic programming](http://athenasc.com/ndpbook.html)** book by D. P. Bertsekas and J. Tsitsiklis (1996).\n",
    "\n",
    "So the neighborhood reached has the same size as that of Approximate Value Iteration.\n",
    "\n",
    "Similar **[error bounds for approximate policy iteration](https://www.aaai.org/Papers/ICML/2003/ICML03-074.pdf)** in weighted $L_p$ norms were provided by R. Munos (2003).  \n",
    "These bounds were later generalized by the study of **[approximate modified policy iteration](https://icml.cc/2012/papers/608.pdf)** by B. Scherrer et al. (2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d92f4a-c53b-4936-87a9-c9437d8efdca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "From now on, we will cast our algorithms as approximate dynamic programming ones, operating on a given function space for Q-functions, part of $\\mathbb{R}^{SA}$, regardless of the nature of $S$ and $A$.\n",
    "</div>\n",
    "\n",
    "The key idea we develop in this chapter is that one can actually *learn* the sequence of AVI functions using interaction samples rather than *calculate* it using a model.\n",
    "\n",
    "<center><img src=\"img/brain.png\" width=\"400px\"></img></center>\n",
    "\n",
    "Although we have introduced a fair amount of abstract concepts, it is important to keep in mind that these maths simply formalize an intuitive cognitive process. By experiencing rewards and punishments, we (humans) incrementally learn to evaluate the outcomes of our actions and then decide to act accordingly. This cognitive process is thus in line with the formalism we have introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09e560-1662-4731-a16d-7e7cd23428de",
   "metadata": {},
   "source": [
    "# Approximate dynamic programming as a sequence of supervised learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202944e-f6e7-48d3-aae4-4a575fa72ca1",
   "metadata": {},
   "source": [
    "**Prerequisite: random variables and expectations**\n",
    "\n",
    "You might remember how we defined the *return* random variable $G^\\pi(s)$ for a policy $\\pi$, in each state $s$, as the $\\gamma$-discounted sum of rewards:\n",
    "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi_t,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "Let us take inspiration from the previous definition and define a *bootstrapped return* $G^\\pi_m(s)$, for $m\\geq 1$, as the random variable:\n",
    "$$G^\\pi_m(s) = \\sum\\limits_{t = 0}^{m-1} \\gamma^t R_t + \\gamma^m Q(S_m, A_m) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi_t,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "\n",
    "For $m\\rightarrow\\infty$, $G^\\pi_m(s)\\rightarrow G^\\pi(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c724fb-be7d-4b22-8fff-aa1d41776565",
   "metadata": {},
   "source": [
    "In this specific section, we shall use a generic $T$ operator on Q-functions, which stands for both $T^\\pi$ and $T^*$. When necessary, we will replace $T$ by either $T^\\pi$ or $T^*$, or by their expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51216b19-8d41-4e36-ae83-f66c9ef84003",
   "metadata": {},
   "source": [
    "Let us define the dynamic programming sequence of functions $Q_{n+1} = T^* Q_n$.  \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Suppose Q-functions belong to some parametrized set of functions $Q(s,a;\\theta)$. \n",
    "The parameter of $Q_n$ is noted $\\theta_n$. \n",
    "We want to approximate $T^*Q_n$ with $Q(s,a;\\theta_{n+1})$. \n",
    "Write an objective function $L_n(\\theta)$ such that $\\theta_{n+1} = \\arg\\min_{\\theta} L_n(\\theta)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876bd0f-af83-4f17-9c73-66dcb5c19b72",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Recall that\n",
    "$$(T^*Q)(s,a) = \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a')\\right].$$\n",
    "\n",
    "Let's write $G_n(s,a)$ the random variable in the expectation above. $G_n(s,a)$ is the sum of what we get by applying $a$ in $s$, which is r(s,a,s'), with $s'$ drawn according to $p(\\cdot|s,a)$), and the biggest value estimate from $s'$ according to $\\theta_n$, discounted by $\\gamma$.\n",
    "\n",
    "For the sake of notation simplicity, let's write, for a given $\\theta_n$: \n",
    "$$y_n(s,a) = \\mathbb{E} \\left[ G_n(s,a) \\right] = \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a';\\theta_{n})\\right].$$\n",
    "\n",
    "Then finding $\\theta_{n+1}$ given $\\theta_n$ is the regression problem that minimizes the objective:\n",
    "$$L_n(\\theta) = \\left\\| y_n(s,a) - Q(s,a;\\theta) \\right\\|.$$\n",
    "\n",
    "Note that this objective function depends on the chosen norm. If this function can be optimized and goes to zero, then we have found the true $Q_{n+1}$. If not, then we have found an approximation of it in the norm used to define $L_n$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6808083-2b05-4305-b7ca-735eb198f9ce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Use the quadratic loss function to define the previous optimization problem as a risk minimization one. Then write the gradient of this risk with respect to the Q-function's parameters. Use this to introduce a stochastic gradient descent method to find $\\theta_{n+1}$.  \n",
    "Use the (weighted) L2 norm in the loss of the regression problem above, then write the gradient of the loss with respect to the regressor's parameters. Use this to introduce a stochastic gradient descent method to find $\\theta_{n+1}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd614a-699d-4839-8ebe-dac154e4274d",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The regression problem we consider takes inputs in $S\\times A$ and outputs in $\\mathbb{R}$. \n",
    "To define the risk, we need to introduce the distribution over inputs and outputs. Let us write a distribution $\\rho$ over inputs in $S\\times A$. For a given $s,a$, the law of $G_n(s,a)$ is fully determined by the transition model $p(s'|s,a)$. \n",
    "\n",
    "Our objective function then becomes:\n",
    "\\begin{gather}\n",
    "L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\big( G_n(s,a) - Q(s,a;\\theta) \\big)^2 \\right],\\\\\n",
    "\\textrm{with } G_n(s,a) = r(s,a,s') + \\gamma \\max_{a'} Q(s',a';\\theta_{n}),\\textrm{ with } s' \\sim p(\\cdot|s,a).\n",
    "\\end{gather}\n",
    "\n",
    "In the expression above, $\\rho$ is a distribution over the state-action space. Often, it is considered to be the behavior distribution, that is the distribution of samples under the current behavior policy, like $\\epsilon$-greedy. Note that this choice is debatable.\n",
    "\n",
    "So the gradient of this loss is:\n",
    "\\begin{gather}\n",
    "\\nabla_\\theta L_n(\\theta) = \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\big( y_n(s,a) - Q(s,a;\\theta) \\big) \\nabla_\\theta Q(s,a;\\theta) \\right]\\\\\n",
    "\\textrm{with } y_n(s,a) = \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a';\\theta_{n})\\right].\n",
    "\\end{gather}\n",
    "\n",
    "And when we wrap all this together:\n",
    "$$\\nabla_\\theta L_n(\\theta) = \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\left( \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a';\\theta_{n})\\right] - Q(s,a;\\theta) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right]$$\n",
    "\n",
    "$$\\nabla_\\theta L_n(\\theta) = \\mathbb{E}_{\\substack{(s,a) \\sim \\rho(\\cdot)\\\\ s' \\sim p(\\cdot|s,a)}}\\left[ \\left( r(s,a,s') + \\gamma \\max_{a'} Q(s',a';\\theta_{n}) - Q(s,a;\\theta) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right]$$\n",
    "\n",
    "We can build a Monte Carlo estimate of this gradient, given a mini-batch of independently and identically drawn samples $\\left\\{\\left(s_i,a_i,r_i,s'_i\\right)\\right\\}_{i\\in [1,B]}$, with $(s,a) \\sim \\rho(\\cdot)$ and $s' \\sim p(\\cdot | s,a)$:\n",
    "$$\\nabla_\\theta L_n(\\theta) \\approx d_n(\\theta) = \\sum_{i=1}^B \\left[ \\left( r_i + \\gamma \\max_{a'} Q(s_i',a';\\theta_{n}) - Q(s_i,a_i;\\theta) \\right) \\nabla_\\theta Q(s_i,a_i;\\theta) \\right].$$\n",
    "\n",
    "The stochastic gradient descent procedure builds a sequence of parameter values $\\theta_k$ such that:\n",
    "$$\\theta_{k+1} \\leftarrow \\theta_{k} - \\alpha d_n(\\theta_{k})$$\n",
    "\n",
    "By repeating such gradient steps, one progressively minimizes $L_n(\\theta)$ and finds $\\theta_{n+1}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed15a98",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Learning for policy evaluation\n",
    "\n",
    "## Sampling the return to learn a value function\n",
    "\n",
    "Let's step aside from dynamic programming for a second and return to the definition of $Q^\\pi$. \n",
    "Recall that evaluating $Q^\\pi(s,a)$ is estimating the mathematical expectation of $G^\\pi(s,a)$.\n",
    "\n",
    "Stochastic approximation theory tells us that, for a given $s,a$ pair, given a series $g^\\pi_t$ of independent realizations of $G^\\pi(s,a)$, the sequence\n",
    "$q_{t+1} = q_t + \\alpha_t \\left(g^\\pi_t - q_t\\right)$\n",
    "converges to $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$, if the sequence of $\\alpha_t$ respects the Robbins-Monro conditions ($\\sum_t \\alpha_t = \\infty$ and $\\sum_t \\alpha_t^2 < \\infty$).\n",
    "\n",
    "**An intuitive reminder on Stochastic Approximation.**  \n",
    "\n",
    "    \n",
    "For those unfamiliar with stochastic approximation procedures, we can understand the previous update as: $g^\\pi_t$ are sample estimates of $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$. If I already have an estimate $q_t$ of $\\mathbb{E}\\left(G^\\pi(s,a)\\right)$ and I receive a new sample $g^\\pi_t$, I should \"pull\" my previous estimate towards $g^\\pi_t$. But $g^\\pi_t$ carries a part of noise, so I should be cautious and only take a small step $\\alpha$ in the direction of $g^\\pi_t$.\n",
    "    \n",
    "In turn, the convergence conditions simply state that any value $Q^\\pi(s,a)$ should be reachable given any initial guess $Q(s,a)$, no matter how far is this first guess from $Q^\\pi(s,a)$; hence the $\\sum\\limits_{t=0}^\\infty \\alpha_t = \\infty$. However, we still need the step-size to be decreasing so that we don't start oscillating around $Q^\\pi(s,a)$ when we get closer; so to insure convergence we impose $\\sum\\limits_{t=0}^\\infty \\alpha_t^2 < \\infty$.\n",
    "\n",
    "So this provides us with a way to estimate $Q^\\pi(s,a)$ from experience samples rather than from a model.  \n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Policy evaluation as stochastic approximation**  \n",
    "If we can obtain independent realizations $g^\\pi(s,a)$ of $G^\\pi(s,a)$ in all $s,a$, we can perform stochastic approximation updates of $Q$ under the form:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left(g^\\pi(s,a) - Q(s,a)\\right).$$\n",
    "Then $Q$ converges to $Q^\\pi$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420d4c7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The stochastic gradient descent formulation.**\n",
    "\n",
    "A more modern formulation of Stochastic Approximation is Stochastic Gradient Descent. So we will slightly generalize the formulation above.  \n",
    "\n",
    "$Q^\\pi$ is the function that minimizes\n",
    "$$L(Q) = \\frac{1}{2} \\int_{S\\times A} \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right)\\right]^2 dsda.$$\n",
    "\n",
    "Recall that in the most general case, $Q$ is a function, but for the sake of clarity, we will momentarily suppose that $S\\times A$ is finite and thus $Q$ is equivalent to the vector of all $Q(s,a)$ values.\n",
    "\n",
    "Then, minimizing $L(Q)$ can be done via gradient descent:\n",
    "$$\\nabla_Q L(Q) = \\int_{S\\times A} \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right) \\right] \\nabla_Q Q(s,a) dsda.$$\n",
    "\n",
    "Suppose we have a set of independently drawn states and actions $\\left\\{(s_i, a_i)\\right\\}_{i\\in [1,N]}$. Then, this gradient can be approached via a Monte Carlo estimator:\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N \\left[ Q(s_i,a_i) - \\mathbb{E}\\left(G^\\pi(s_i,a_i)\\right)\\right] \\nabla_Q Q(s_i, a_i).$$\n",
    "\n",
    "In our example where $Q$ is the vector of values taken in each state and action, \n",
    "$\\nabla_Q Q(s_i,a_i) = \\left[ \\begin{array}{c} 0\\\\ \\vdots\\\\ 0\\\\ 1 \\\\ 0\\\\ \\vdots\\\\ 0 \\end{array} \\right]$ \n",
    "where the \"1\" is at the position corresponding to $s_i,a_i$ in the vector $Q$.\n",
    "\n",
    "As for Stochastic Approximation, if we can obtain independent realizations $g^\\pi(s_i,a_i)$ of $G^\\pi(s_i,a_i)$, then we can estimate this gradient as:\n",
    "$$d = \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i).$$\n",
    "\n",
    "And thus we have the Stochastic Gradient Descent update:\n",
    "$$Q \\leftarrow Q - \\alpha \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i)$$\n",
    "\n",
    "This update mechanism yields a sequence $Q_t$ of value functions that converges to $Q^\\pi$ if the gradient steps $\\alpha$ respect Robbins-Monro conditions.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Policy evaluation as Stochastic Gradient Descent**  \n",
    "If we can obtain independent realizations $g^\\pi(s,a)$ of $G^\\pi(s,a)$ in all $s,a$, we can perform Stochastic Gradient Descent updates on $Q$:\n",
    "$$Q \\leftarrow Q + \\alpha \\sum_{i=1}^N \\left[ g^\\pi(s_i,a_i) - Q(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i).$$\n",
    "Then $Q$ converges to $Q^\\pi$.\n",
    "</div>\n",
    "\n",
    "Note that if $N=1$, the update above falls back to the Stochastic Approximation update: having a sample in $s_i,a_i$ only updates $Q(s_i,a_i)$.\n",
    "\n",
    "So, overall, if we manage to draw independent samples $g^\\pi(s_i,a_i)$ of $G^\\pi(s,a)$ in all $s,a\\in S\\times A$, we can **learn** the value $Q^\\pi$ (or $V^\\pi$) of policy $\\pi$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8413e9b-9808-4e3e-85c0-1dd13369cc01",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sampling the bootstrapped return: dynamic programming as a sequence of supervised learning problems\n",
    "\n",
    "Now, obtaining a sample $g^\\pi(s_i,a_i)$ of $G^\\pi(s,a)$ might be a bit costly, as it requires running a full trajectory and summing the observed rewards, as we have done in the previous chapters' exercises. It's doable (and is actually the topic of a series of homework exercises) but instead, we might want to sample from the bootstrapped return $G^\\pi_1(s,a,Q)$. This is a lot cheaper since it only requires sampling the next state.\n",
    "\n",
    "Recall that $$G^\\pi_1(s,a,Q) = R_0 + \\gamma Q(S_1, A_1) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_1 \\sim \\pi(S_1),\\\\ S_{1}\\sim p(\\cdot|S_0,A_0),\\\\ R_0 = r(S_0,A_0,S_{1}).\\end{array}$$\n",
    "\n",
    "So sampling from $G^\\pi_1(s,a,Q)$ is done by sampling a next state $s'$ according to $p(s'|s,a)$ and a next action $a'$ according to $\\pi(a'|s')$. The sample is then:\n",
    "$$g^\\pi_1(s,a,Q) = r(s,a,s') + \\gamma Q(s',a').$$\n",
    "\n",
    "Recall also that $$(T^\\pi Q)(s,a) = \\mathbb{E} \\left[ G^\\pi_1(s,a,Q) \\right].$$\n",
    "\n",
    "So sampling from $G^\\pi_1(s,a,Q)$ actually provides samples to learn $T^\\pi Q$.  \n",
    "\n",
    "**If we repeatedly replace $Q$ in $G^\\pi_1(s,a,Q)$ by a value function we have previously learned, we are learning the sequence of approximate dynamic programming functions $Q_{n+1} = \\mathcal{A}T^\\pi Q_n$, where the approximation operator $\\mathcal{A}$ is the operation \"learn from samples of $G^\\pi_1(s,a,Q_n)$\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad8872-1634-4a4e-8f74-1a681654ecba",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Suppose Q-functions belong to some parametrized set of functions $Q(s,a;\\theta)$. The parameter of $Q_n$ is noted $\\theta_n$.  \n",
    "We want to approximate $(T^\\pi Q_n)(s,a)$ with $Q(s,a;\\theta_{n+1})$.  \n",
    "Use the quadratic loss function to define learning $T^\\pi Q_n$ as a risk minimization problem $\\min_\\theta L_n(\\theta)$. Then write the gradient of this risk with respect to the Q-function's parameters. Use this to introduce a stochastic gradient descent method to find $\\theta_{n+1}$.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa5d52-e6c7-4054-b940-41cd7669aef0",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Learning $Q_{n+1}$ is a regression problem. This problem takes inputs in $S\\times A$ and outputs in $\\mathbb{R}$.  \n",
    "To define the risk, we need to introduce the distribution over inputs and outputs. Let us write a distribution $\\rho$ over inputs in $S\\times A$. For a given $s,a$, the probability law of $G^\\pi_1(s,a,Q_n)$ is fully determined by the transition model $p(s'|s,a)$ and the policy $\\pi(a'|s')$. \n",
    "\n",
    "The risk to minimize is then:\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\left( G^\\pi_1(s,a,Q_n) - Q(s,a;\\theta) \\right)^2 \\right].$$\n",
    "\n",
    "In the expression above, $\\rho$ is a distribution over the state-action space. Intuitively, it should cover the important parts of the state-action space.\n",
    "\n",
    "The gradient of this risk is:\n",
    "$$\\nabla_\\theta L_n(\\theta) = -\\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\left( G^\\pi_1(s,a,Q_n) - Q(s,a;\\theta) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right].$$\n",
    "\n",
    "Recall that we can sample from $G^\\pi_1(s,a,Q_n)$ simply by sampling a next state $s'$ according to $p(s'|s,a)$ and a next action $a'$ according to $\\pi(a'|s')$. The sample is then:\n",
    "$$g^\\pi_1(s,a,Q_n) = r(s,a,s') + \\gamma Q(s',a';\\theta_{n}).$$\n",
    "\n",
    "So when we wrap all this together:\n",
    "$$\\nabla_\\theta L_n(\\theta) = -\\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\left( \\mathbb{E}_{\\substack{s' \\sim p(\\cdot|s,a)\\\\ a'\\sim \\pi(a'|s')}} \\left[r(s,a,s') + \\gamma Q(s',a';\\theta_{n})\\right] - Q(s,a;\\theta) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right]$$\n",
    "\n",
    "$$\\nabla_\\theta L_n(\\theta) = -\\mathbb{E}_{\\substack{(s,a) \\sim \\rho(\\cdot)\\\\ s' \\sim p(\\cdot|s,a)\\\\ a'\\sim \\pi(a'|s')}}\\left[ \\left( r(s,a,s') + \\gamma Q(s',a';\\theta_{n}) - Q(s,a;\\theta) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right]$$\n",
    "\n",
    "We can build a descent direction as a Monte Carlo estimate of $-\\nabla_\\theta L_n(\\theta)$, given a mini-batch of independently and identically drawn samples $\\left\\{\\left(s_i,a_i,r_i,s'_i\\right)\\right\\}_{i\\in [1,B]}$, with $(s,a) \\sim \\rho(\\cdot)$ and $s',a' \\sim p(s' | s,a)\\pi(a'|s')$:\n",
    "$$d_n(\\theta) = \\frac{1}{B} \\sum_{i=1}^B \\left[ \\left( r_i + \\gamma Q(s_i',a';\\theta_{n}) - Q(s_i,a_i;\\theta) \\right) \\nabla_\\theta Q(s_i,a_i;\\theta) \\right].$$\n",
    "\n",
    "The stochastic gradient descent procedure builds a sequence of parameter values $\\theta_k$ such that:\n",
    "$$\\theta_{k+1} \\leftarrow \\theta_{k} - \\alpha d_n(\\theta_{k})$$\n",
    "\n",
    "By repeating such gradient steps, one progressively minimizes $L_n(\\theta)$ and finds $\\theta_{n+1}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5c6de-5bc6-4a36-95ed-4bca14dd3c8a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that minimizing the empirical risk does not require it to be differentiable with respect to the parameters of $Q$. For instance, one could use decision trees or **[random forests](https://link.springer.com/article/10.1023/A:1010933404324)** for this purpose.  \n",
    "Note also that other objective functions can be used instead of the empirical risk, like regularized risk measures (as in **[support vector regression](https://link.springer.com/article/10.1023/B:STCO.0000035301.49549.88)** for instance).\n",
    "\n",
    "The goal of this section was to state an important idea: \n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Approximate dynamic programming can be tackled as a sequence of supervised learning problems.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441aa08-f863-4c1f-8cbc-4fb4ad49ae0d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Temporal differences\n",
    "\n",
    "Let's apply what we have just written using stochastic approximation as a learning procedure, that is with a mini-batch size $B=1$ and samples drawn on-the-fly from interaction with the system to control, at each time step. To fix ideas, we will also write everything for finite state and action spaces MDPs. Thus, in this section, we will repeat (although maybe in a simpler way) many things we have stated in the previous one for the more general case of stochastic gradient descent.\n",
    "\n",
    "Consider the sample $(s_t,a_t,r_t,s_{t+1})$ obtained at time $t$.\n",
    "\n",
    "Once this transition is over we can update our knowledge of $Q(s_t, a_t)$ by using $r_t+\\gamma Q(s_{t+1},\\pi(s_{t+1}))$. This estimate uses $Q(s_{t+1},\\pi(s_{t+1}))$ to *bootstrap* the estimator of $Q(s_t, a_t)$.\n",
    "\n",
    "This idea was first introduced in R. Sutton's **[Learning to predict by the methods of temporal differences](https://link.springer.com/article/10.1007/BF00115009)** article.\n",
    "\n",
    "The bootstrapped sample $g^\\pi_t$ of $Q^\\pi(s_t,a_t)$ is obtained by summing $r_t$ and $\\gamma Q_t(s_{t+1}, \\pi(s_{t+1}) )$:\n",
    "$$g_t = r_t + \\gamma Q_t(s_{t+1}, \\pi(s_{t+1})).$$\n",
    "\n",
    "Note that in the expression above, we have used $Q_t$ to emphasize that we use the function $Q$ as it was at time step $t$, to define the target $g^\\pi_t$ used in the update that will provide $Q_{t+1}$.\n",
    "\n",
    "Formally, this comes directly from the evaluation operator. Let's rewrite $T^\\pi$ in terms of random variables.\n",
    "$$(T^\\pi Q)(s,a) = \\mathbb{E}_{R,S'}\\left[ R + \\gamma Q(S', \\pi(S')) \\right]$$\n",
    "\n",
    "Since $Q^\\pi$ is the fixed point of $T^\\pi$, by taking $g_t = r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1}))$ we are taking one stochastic approximation step in the direction of $T^\\pi Q_t$. \n",
    "\n",
    "**Bootstrapping** (in this particular context) is the operation of using the value of $Q_t(s_{t+1},\\pi(s_{t+1}))$ in the update of $Q$.\n",
    "\n",
    "Then the stochastic approximation update becomes what is called the **TD(0) update**:\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**TD(0) update:**  \n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left(r_t + \\gamma Q(s_{t+1}, \\pi(s_{t+1})) - Q(s_t,a_t)\\right).$$\n",
    "    \n",
    "This update consists in taking one stochastic approximation step in the direction of $T^\\pi Q$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a14db5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's insist on this point:  \n",
    "TD(0) does not directly solve $Q=\\mathbb{E}\\left[\\sum_t\\gamma^t R_t \\right]$ (this is what other methods, called *Monte Carlo*, do --- see the homework for details on Monte Carlo methods). Instead, it implements stochastic approximation on top of the repeated application of the $T^\\pi$ operator. So it solves $Q_{n+1} = T^\\pi Q_n$. At each step $t$, it takes the current value function $Q_t$, draws one or several samples from $T^\\pi Q_t$ and approximates $T^\\pi Q_t$ by taking one step of gradient descent from $Q_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e82559",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\delta_t=r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1})) - Q_t(s_t,a_t)$ is called the prediction **temporal difference** (hence the name of the algorithm - the \"0\" won't be explained here). It is the difference between our estimate $Q_t(s_t,a_t)$ *before* obtaining the information of $r_t$, and the bootstrapped value $r_t + \\gamma Q_t(s_{t+1},\\pi(s_{t+1}))$.\n",
    "<div class=\"alert alert-success\"><b>Temporal difference:</b>\n",
    "$$\\delta=r + \\gamma Q(s',\\pi(s')) - Q(s,a)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0a84a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now it seems obvious that if some state-action pair $s,a$ is never visited, then no update of its $Q(s,a)$ can ever take place. Therefore, for the TD(0) update to converge, we need to guarantee that all state action pairs will be visited frequently enough for $Q$ to converge to $Q^\\pi$.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>TD(0) temporal difference update on $Q$-functions:</b><br>\n",
    "For a sample $(s,a,r,s')$, the temporal difference is:\n",
    "$$\\delta = r + \\gamma Q(s',\\pi(s')) - Q(s,a)$$\n",
    "And the TD update is:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma Q(s',\\pi(s')) - Q(s,a) \\right]$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, and under the Robbins-Monro conditions, this procedure converges to $Q^\\pi$.\n",
    "</div>\n",
    "\n",
    "Interestingly, this algorithm puts restrictions on the policy we apply when interacting with the environment. We will call such a policy a **behavior policy**. The behavior policy and the policy being learned might be different (in the case of TD(0), this even is an obligation since we need to enforce visitation of all state-action pairs).\n",
    "\n",
    "Vocabulary: **Off-policy** evaluation algorithms can use a behavior policy that is different than the policy being evaluated.\n",
    "\n",
    "Do you recall the distribution $\\rho$ on states and actions in the previous section? It is the distribution of states and actions under the behavior policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58916695",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together):**  \n",
    "Let's implement TD(0) on $Q$-functions.  \n",
    "To insure that all states and actions are sampled infinitely often, we take a behavior policy that acts randomly in each state.  \n",
    "We take $\\gamma=0.9$ and run the algorithm for $10^6$ time steps.   \n",
    "To keep things simple, we take a constant $\\alpha=0.01$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b77e6-acfb-47dc-be07-56ffff2c5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first recall the model-based Q-function, so we can compare.\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.RL2_exercise6 import policy_eval_iter_mat2\n",
    "from solutions.RL2_exercise1 import Q_from_V\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n))\n",
    "V_pi0, residuals = policy_eval_iter_mat2(pi0,1e-4,10000)\n",
    "Q_pi0 = Q_from_V(V_pi0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc911c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now let's implement TD(0)\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "max_steps=int(1e6)\n",
    "Qtd = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "error = np.zeros((max_steps))\n",
    "x,_ = env.reset()\n",
    "for t in range(max_steps):\n",
    "    a = np.random.randint(4)\n",
    "    y,r,d,_,_ = env.step(a)\n",
    "    Qtd[x,a] = Qtd[x,a] + alpha * (r+gamma*Qtd[y,fl.RIGHT]-Qtd[x,a])\n",
    "    error[t] = np.max(np.abs(Qtd-Q_pi0))\n",
    "    if d==True:\n",
    "        x,_ = env.reset()\n",
    "    else:\n",
    "        x=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f2231",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the difference between Qtd and Q_pi0\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Max error:\", np.max(np.abs(Qtd-Q_pi0)))\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f5e2e-2719-4663-85d7-571f3917d2ed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Learning optimal value functions\n",
    "\n",
    "## Approximate Value Iteration as a sequence of supervised learning problems\n",
    "\n",
    "Extending the ideas developped in the previous section is quite straightforward when we remember that value iteration is actually the alternance of applying a *greediness operator* on a Q-function to define $\\pi$, then applying $T^\\pi$ to $Q$.\n",
    "\n",
    "Let's recall definitions of the previous chapter:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Greediness operator**  \n",
    "For deterministic policies:\n",
    "$$\\pi \\in \\mathcal{G} Q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{a\\in A} Q(s,a)$$\n",
    "\n",
    "This can be extended to stochastic policies:\n",
    "$$\\pi \\in \\mathcal{G} Q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{\\pi \\in \\Delta_A} \\mathbb{E}_{a\\sim\\pi} \\left[Q(s,a)\\right]$$\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Value iteration**\n",
    "$$\\pi_n \\in \\mathcal{G} Q_n, \\quad Q_{n+1} = T^{\\pi_n} Q_n.$$\n",
    "</div>\n",
    "\n",
    "Then, $Q^*$ is the limit reached by the sequence of $Q_n$ functions.\n",
    "\n",
    "So, if we use an approximation procedure $\\mathcal{A}$ which minimizes the empirical risk when learning $T^{\\pi_n} Q_n$, we are learning $Q_{n+1}$. Then we can define $\\pi_{n+1} \\in \\mathcal{G} Q_{n+1}$ and repeat the learning procedure with $T^{\\pi_{n+1}} Q_{n+1}$.\n",
    "\n",
    "The key difference with what we wrote for the evaluation equation is that the policy being evaluated at each step of the sequence now changes, as it is defined as greedy with respect to the last learned value function.\n",
    "\n",
    "Let us sketch an algorithm out of this:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Approximate value iteration as a sequence of risk minimization problems**  \n",
    "$$\\pi_n \\in \\mathcal{G} Q_n,$$\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\left( G^{\\pi_n}_1(s,a,Q_n) - Q(s,a;\\theta) \\right)^2 \\right],$$\n",
    "$$Q_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta).$$\n",
    "</div>\n",
    "\n",
    "We won't go any further in this section and in particular we won't directly implement the algorithm above. The homework will guide you to implement it with random forests and future classes will implement it with neural networks (yielding the famous **[Deep Q-networks](https://www.nature.com/articles/nature14236)** algorithm). The point of this section is really to cast the resolution of the Bellman equation as the very generic problem of a sequence of supervised learning problems. So we repeat what was already written in the previous section:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Approximate dynamic programming can be tackled as a sequence of supervised learning problems.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a73dc-edcb-40d7-b5c8-a23a2c0be8dc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Approximate Value Iteration as Stochastic Approximation: Q-learning\n",
    "\n",
    "In this section we will adapt the reasoning we had earlier with TD learning: we will implement the approximate value iteration algorithm above using stochastic approximation as an approximation procedure, in discrete state and action space MDPs, and with deterministic policies. \n",
    "\n",
    "Note that if we use deterministic policies, then we don't even need to write $\\pi_n$ anymore: we can directly replace any mention of $\\pi_n(s)$ with an $\\arg\\max_a Q_n(s,a)$, and any mention of $Q_n(s,\\pi_n(s))$ by $\\max_a Q_n(s,a)$. \n",
    "\n",
    "So directly adapt the idea of temporal difference learning to the approximate value iteration case.  \n",
    "\n",
    "In this case, we want to learn $T^* Q_t$ (instead of $T^\\pi Q_t$) so our samples are:\n",
    "$$g_t = r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a').$$\n",
    "\n",
    "And the learning algorithm becomes the famous **Q-learning** algorithm, introduced by C. J. Watkins in his [PhD thesis](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf) in 1989:\n",
    "<div class=\"alert alert-success\"><b>Q-learning</b><br>\n",
    "For a sample $(s,a,r,s')$, the temporal difference is:\n",
    "$$\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$$\n",
    "And the TD update is:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, and under the Robbins-Monro conditions, this procedure converges to $Q^*$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8228281",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To implement a Q-learning algorithm, one needs to decide on a behavior policy. As for TD(0), Q-learning will converge to $Q^*$, provided that all states and actions are visited infinitely often. It is actually notable that $Q$ converges to $Q^*$ even if the behavior policy does not. But it also looks like a waste of computational resources to keep exploring uniformly around the starting state.\n",
    "\n",
    "This tradeoff between exploring new actions and exploiting what has already been inferred in $Q$ is called the **exploration versus exploitation tradeoff**. It is a crucial problem that strongly affects the ability of the algorithm to discover new, interesting rewards.\n",
    "\n",
    "Here we will implement a rather naive tradeoff strategy called an $\\epsilon$-greedy behavior. It consists in picking the $Q$-greedy action with probability $1-\\epsilon$ and a random action with probability $\\epsilon$.\n",
    "\n",
    "$\\epsilon$ will start at 1 and then will periodically be divided by 2.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**\n",
    "\n",
    "Write a function that picks an epsilon-greedy action.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ae3b95",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(Q, s, epsilon):\n",
    "    env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "    a = np.argmax(Q[s,:])\n",
    "    if(np.random.rand()<=epsilon): # random action\n",
    "        aa = np.random.randint(env.action_space.n-1)\n",
    "        if aa==a:\n",
    "            a=env.action_space.n-1\n",
    "        else:\n",
    "            a=aa\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215aa9d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**\n",
    "\n",
    "Write a Q-learning algorithm on FrozenLake. Keep track of the error w.r.t. $Q^*$ and the number of times each state-action pair is visited.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2efaa7-fad8-4d78-905d-c972b69784f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recall the optimal value function from the previous chapter\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "from solutions.RL2_exercise7 import value_iteration\n",
    "from solutions.RL2_exercise1 import Q_from_V\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(Vstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e2628b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0.08970060799982832\n"
     ]
    }
   ],
   "source": [
    "# Implement Q-learning. Let's restart from the previous Qpi.\n",
    "Qql = np.zeros((env.observation_space.n,env.action_space.n)) #Q_pi0\n",
    "count = np.zeros((env.observation_space.n,env.action_space.n)) # to track update frequencies\n",
    "max_steps = int(1e6)\n",
    "epsilon = 1\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "error = np.zeros((max_steps))\n",
    "x,_ = env.reset()\n",
    "for t in range(max_steps):\n",
    "    if((t+1)%int(1e6)==0):\n",
    "        epsilon = epsilon/2\n",
    "    a = epsilon_greedy(Qql,x,epsilon)\n",
    "    y,r,d,_,_ = env.step(a)\n",
    "    Qql[x][a] = Qql[x][a] + alpha * (r+gamma*np.max(Qql[y][:])-Qql[x][a])\n",
    "    count[x][a] += 1\n",
    "    error[t] = np.max(np.abs(Qql-Qstar))\n",
    "    if d==True:\n",
    "        x,_ = env.reset()\n",
    "    else:\n",
    "        x=y\n",
    "\n",
    "# Q-learning's final value function and policy\n",
    "print(\"Max error:\", np.max(np.abs(Qql-Qstar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e362e69-083e-49e2-bfec-97d289cc9f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0.08970060799982832\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the difference between Qtd and Q_pi0\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Max error:\", np.max(np.abs(Qql-Qstar)))\n",
    "plt.figure()\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137060b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**\n",
    "\n",
    "Display the visitation frequency</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d290a7e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from solutions.RL1_utility_functions import actions, to_row_col\n",
    "\n",
    "count_map = np.zeros((env.unwrapped.nrow, env.unwrapped.ncol, env.action_space.n))\n",
    "for a in range(env.action_space.n):\n",
    "    for x in range(env.observation_space.n):\n",
    "        row,col = to_row_col(x)\n",
    "        count_map[row, col, a] = count[x,a]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4)\n",
    "for a in range(env.action_space.n):\n",
    "    name = \"a = \" + actions[a]\n",
    "    axs[a].set_title(name)\n",
    "    axs[a].imshow(np.log(count_map[:,:,a]+1), interpolation='nearest')\n",
    "    #print(\"a=\", a, \":\", sep='')\n",
    "    #print(count_map[:,:,a])\n",
    "plt.show()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0771a77",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (together)**  \n",
    "Display the final policy.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4bad45",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final epsilon: 0.5\n",
      "Greedy Q-learning policy:\n",
      "←↑←↑\n",
      "←←→←\n",
      "↑↓←←\n",
      "←→↓←\n",
      "Optimal policy:\n",
      "←↑←↑\n",
      "←←←←\n",
      "↑↓←←\n",
      "←→↓←\n"
     ]
    }
   ],
   "source": [
    "from solutions.RL2_exercise2 import greedyQpolicy\n",
    "from solutions.RL2_utility_functions import print_policy\n",
    "\n",
    "print(\"Final epsilon:\", epsilon)\n",
    "pi_ql = greedyQpolicy(Qql)\n",
    "print(\"Greedy Q-learning policy:\")\n",
    "print_policy(pi_ql)\n",
    "pi_star = greedyQpolicy(Qstar)\n",
    "print(\"Optimal policy:\")\n",
    "print_policy(pi_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fdefc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155e387",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Previous chapters have shown how to charaterize and find optimal policies given the MDP model. We have built on the results of Approximate Dynamic Programming to **learn** optimal value functions from interaction samples.\n",
    "\n",
    "So we have built the third stage of our three-stage rocket defined in the introduction chapter.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**How do we learn an optimal strategy?**  \n",
    "To learn an optimal strategy, we rely on a stochastic approximation of $Q^*$, given samples drawn from the MDP. This stochastic approximation of $Q^*$ is actually a stochastic approximation of the $Q_n$ sequence of approximate value iteration. In the end, we need to find good approximation architectures and to control the exploration versus exploitation tradeoff.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0113d2-e37b-465a-9467-4d6a6ffcf89f",
   "metadata": {},
   "source": [
    "Let's take a step back and provide a more general summary this far.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**What is Reinforcement Learning?**  \n",
    "RL is the discipline that studies the *learning* process of optimal control policies in the MDP framework.  \n",
    "Its roots overlap Cognitive Psychology, Control Theory, Artificial Intelligence, Machine Learning.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**What are the building blocks of RL?**  \n",
    "RL is built upon the framework of Markov Decision Processes.  \n",
    "It draws from the characterization of optimal policies that maximize a given criterion, notably through Bellman's equations.  \n",
    "It learns (notably through stochastic approximation or SGD) solutions to these equations using interaction samples.\n",
    "</div>\n",
    "\n",
    "Of course we have barely touched the surface of RL for now. We haven't explored the weaker notion of optimality and the direct policy search methods sketched out in chapter 1 for instance. The overall goal of these first chapters was to acquire a common vocabulary and set of concepts, so that you become comfortable with the objects often manipulated in RL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e2bfe-7bf5-4e25-afc3-707d315e2d5c",
   "metadata": {},
   "source": [
    "## Three intrinsic challenges in Reinforcement Learning\n",
    "\n",
    "From here, we can identify three challenges which make the RL problem intrinsically difficult:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Intrinsic challenges in RL:**  \n",
    "- function approximation,\n",
    "- the improvement problem,\n",
    "- the exploration versus exploitation trade-off.\n",
    "</div>\n",
    "\n",
    "These three challenges are quite independent and we could study them in any order.\n",
    "\n",
    "As we have seen, **function approximation** is key in finding good policies. Although function approximation does not intrinsically require stochastic gradient descent, the interplay between Deep Learning and Reinforcement Learning has triggered major advances in RL. \n",
    "Chapter 4 is dedicated to this topic and will lead us to manipulate function approximators in AVI, including deep neural networks. \n",
    "\n",
    "The ideas we developped in this class relied on estimating value functions to deduce greedy policies. Finding such greedy policies was made easy because actions were discrete. But **finding improving policies** is actually a challenge in itself. This problem is present both when one searches for a greedy action with respect to $Q$, and when one directly aims at solving the $\\max_\\pi J(\\pi)$ problem without going through the proxy of the optimality equation. Chapter 5 will take us towards the realm of continuous actions in AVI. Chapter 6 will tackle direct policy search and resolution of the $\\max_\\pi J(\\pi)$ problem, notably through policy gradient algorithms.\n",
    "\n",
    "Behavior policies are a cornerstone of RL: which action should one take to obtain informative samples? Should one explore uniformly the environment or rather follow a policy that takes the system towards promising states before exploring more agressively? This is called the **tradeoff between exploration and exploitation**. Chapters 7 and 8 will be dedicated to properly studying this tradeoff through the theory of stochastic bandits, which leads to the UCT and Monte Carlo Tree Search algorithms that are at the root of [alphaGo](https://www.youtube.com/watch?v=WXuK6gekU1Y) and the subsequent alphaZero algorithms.  \n",
    "\n",
    "## Subtopics in RL\n",
    "\n",
    "Beyond these three intrinsic challenges, there are countless, context-dependent, open questions in RL, that form a span of specific questions:\n",
    "- Hierarchical RL\n",
    "- RL from human feedback\n",
    "- World (surrogate) models\n",
    "- Multi-agent RL\n",
    "- Partially observable MDPs\n",
    "- Robust RL\n",
    "- Offline RL\n",
    "- Consolidation and Transfer in RL\n",
    "- Causal RL\n",
    "- and many more (not counting all the application fields)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920e970-db85-44be-961d-a59de7e4f3cb",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1817a155",
   "metadata": {},
   "source": [
    "## Homework: variations on TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08af4f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**\n",
    "Write a function `TD_Qeval` that runs TD(0) on tabular $Q$ functions for a discrete state-action environment and a given policy (it's almost the same code than in section 7.1). Add an option for providing the true $Q$ function and monitoring the error along training. Use the code below to compute the $Q$ function of the policy that always goes right in FrozenLake and plot the evolution of the error between `Qtrue` (model-based computations of previous sections) and $Q$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL3_exercise5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ac099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "pi0 = fl.RIGHT*np.ones((env.observation_space.n),dtype=np.int)\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "Qpi0 = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "Qpi0, error = TD_Qeval(pi=pi0, max_steps=3000000, env=env, alpha=alpha, gamma=gamma, Qpi=Qpi0, Qtrue=Qpi_sequence[-1])\n",
    "print(\"Max error:\", np.max(np.abs(Qpi0-Qpi0true)))\n",
    "plt.plot(error)\n",
    "plt.figure()\n",
    "plt.semilogy(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68231ab3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "We wrote the derivation of (batch, SGD) TD(0) using $Q$ functions defined on the discrete $S\\times A$ space, and taking the gradient with respect to the vector $Q$. Write the same derivation with a parametric state value function $V_\\theta$, then with a parametric state-action value function $Q_\\theta$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070395c4",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We have $V^\\pi(s) = \\mathbb{E} (G^\\pi(s))$. So we consider samples $g^\\pi(s)$ of $G^\\pi(s)$.\n",
    "\n",
    "The loss function becomes $L(\\theta) = \\int_S \\left[ V_\\theta(s) - \\mathbb{E}\\left(G^\\pi(s)\\right)\\right]^2 ds$.\n",
    "    \n",
    "The gradient estimator becomes:\n",
    "$$d = \\sum_{i=1}^N \\left[ V_\\theta(s_i) - g^\\pi(s_i)\\right] \\nabla_\\theta V_\\theta(s_i).$$\n",
    "\n",
    "And so the update becomes:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ g^\\pi(s_i) - V_\\theta(s_i)\\right] \\nabla_\\theta V_\\theta(s_i)$$\n",
    "    \n",
    "For a parametric $Q_\\theta$:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ g^\\pi(s_i,a_i) - Q_\\theta(s_i,a_i)\\right] \\nabla_\\theta Q_\\theta(s_i).$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed33aab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "We derived the TD(0) algorithm for $Q$ functions. Let us draw inspiration from the previous exercise and the derivation of this section to write TD(0) on $V$ functions.\n",
    "- First, recall $T^\\pi$ in terms of an expectation over random variables $R$ and $S'$.\n",
    "- Then define a bootstrapped sample $g_t$ of this expectation.\n",
    "- Finally write the TD(0) SGD update on parametric state value functions, and the corresponding temporal difference.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d4ec4",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- $T^\\pi$ operator: $(T^\\pi V)(s) = \\mathbb{E}_{R,S'}\\left[ R + \\gamma V(S') \\right]$\n",
    "- Bootstrap sample: $g_t = r_t + \\gamma V_t(s_{t+1})$.\n",
    "- The TD(0) update is $\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ r_i + \\gamma V_\\theta(s_{i+1}) - V_\\theta(s_i) \\right] \\nabla_\\theta V_\\theta(s_i)$.  \n",
    "    The temporal difference is $\\delta = r_t + \\gamma V(s_{t+1}) - V(s_t)$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba338d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "When deriving TD(0) on $Q$ functions, we wrote that we needed to enforce the visitation of every state-action pair when obtaining samples from the MDP. Is it still the case for TD(0) on $V$ functions? Is TD(0) on $V$ functions an off-policy algorithm?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3c801",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We need $g_t$ to be a sample of $G^\\pi(s_t) = R_t + \\gamma V(S_{t+1})$. So,  $r_t$ should be a sample of $R_t$, that is the reward obtained by taking action $\\pi(s_t)$ in $s_t$. Additionally, $S_{t+1}$ should be drawn according to $p(\\cdot | s_t,\\pi(s_t)$. So the behavior policy **needs** to be $\\pi$, otherwise the samples lose all meaning. TD(0) on $V$ functions is an **on-policy** algorithm: it's behavior policy is constrained to be the one under evaluation.\n",
    "\n",
    "One could remark that since the policy applied is $\\pi$, we cannot guarantee that all states will be visited. However, the visited states will be those reachable by $\\pi$, from the initial state. Refer to section 5's exercise on the stationary distribution for a discussion on the reachable states and their visitation frequency.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b0785",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Consider the policy that always moves right in FrozenLake and implement a TD(0) estimation for $V^\\pi$.  \n",
    "We can tolerate a 0.001 error on $V^\\pi$ so, to keep things simple, we set $\\alpha=0.001$.  \n",
    "Take $\\gamma = 0.9$ and run the algorithm for 2000000 time steps.  \n",
    "After each episode, compare the value function obtained with that computed with the model-based approaches of section 6.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d00d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL3_exercise2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b014cc",
   "metadata": {},
   "source": [
    "In general, we will call $\\beta$ the *behavior policy*. It is the policy being applied to interact with the environment. Off-policy evaluation algorithms can use a behavior policy that is different than the policy being evaluated. We will call *behavior distribution* the distributions $\\rho^\\beta(s)$ over states and $\\rho^\\beta(s,a)$ over state-action pairs, induced by applying $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015bf81",
   "metadata": {},
   "source": [
    "## Homework: delayed updates and experience replay for TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95542003",
   "metadata": {},
   "source": [
    "We have seen that TD(0), at each time step, takes a gradient step in the direction of $T^\\pi Q$.\n",
    "\n",
    "The result of this gradient step is an approximation of $T^\\pi Q$.\n",
    "\n",
    "We saw in section 6 that despite an approximation operator $\\mathcal{A}$ with controlled error, the sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$ still converged to a neighborhood of $Q^*$. The same result holds for the sequence $Q_{n+1} = \\mathcal{A} T^\\pi Q_n$.\n",
    "\n",
    "One single step of stochastic gradient descent makes for a poor approximator. Given a fixed function $Q_n$, if we repeat a certain number $C$ of such gradient steps, we can hope to obtain a better estimate of $T^\\pi Q_n$. So there is an interest in keeping two $Q$ functions. The first is the current estimator, which plays the role of $Q_n$, upon which we apply $T^\\pi$, and which we call the *target* function $Q^-$. The second is the one we actually optimize upon and which aims at approximating $T^\\pi Q^-$; we write it $Q$. Every $C$ gradient steps, we replace $Q^-$ by $Q$ and repeat.\n",
    "\n",
    "Consequently, this procedure of **delayed updates** trades off advancing in the $Q_{n+1} = \\mathcal{A} T^\\pi Q_n$ sequence for better approximation properties for $\\mathcal{A}$.\n",
    "\n",
    "This makes more apparent the remark made before that TD(0) actually solves the $Q_{n+1} = T^\\pi Q_n$ sequence and thus successively minimizes a sequence of losses:\n",
    "$$L_n(Q) = \\| Q - T^\\pi Q_n \\|_2.$$\n",
    "The loss changes everytime we replace $Q_n$ by $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028c154",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "What's the value of $C$ in vanilla TD(0)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e97cdb",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Vanilla TD(0) replaces $Q^-$ by $Q$ at every step, so $C=1$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17440538",
   "metadata": {},
   "source": [
    "We have written several times the update of TD(0) with a batch of sampels. But in order to be able to perform stochastic gradient steps using more than one sample, one needs to keep samples in memory.\n",
    "\n",
    "Recall the loss we defined to introduce the stochastic gradient update:\n",
    "$$L(Q) = \\int_{S\\times A} \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right)\\right]^2 ds da.$$\n",
    "\n",
    "Recall also that $d = \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i)$ is an unbiased estimate of $\\nabla_Q L(Q)$ only if the $g^\\pi(s_i,a_i)$ are drawn **independently** and **identically** according to the distribution of $G^\\pi(s,a)$.\n",
    "\n",
    "This last condition can only be verified if \n",
    "1. the $s_i,a_i$ are drawn independently of each other and always according to the same distribution $\\rho(s,a)$, and\n",
    "2. given $s_i,a_i$, the realizations $g^\\pi(s_i,a_i)$ are drawn independently of each other and according to the distribution of $G^\\pi(s_i,a_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6eaa2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "What do you think? Is condition 1 verified in vanilla TD(0)? What about condition 2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c538b",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Let's write $(s_i,a_i,r_i,s'_i)$ the $i$th sample and $\\beta$ the behavior policy.\n",
    "    \n",
    "Condition 1 is not verified for vanilla TD(0). It is true that if the behavior policy is constant, then on average the state samples $s_i$ are drawn **identically**, according this policy's stationary state distribution $\\rho^\\beta(s)$ and the action samples are drawn according to $\\beta(\\cdot | s)$. However, successive samples are not drawn **independently** by definition, since $\\mathbb{P}(S_{t+1})$ is actually conditioned by $S_t$ and $A_t\\sim\\beta(\\cdot | S_t)$.\n",
    "\n",
    "    \n",
    "Condition 2, on the other hand is easier to verify for TD(0) updates: since the reward $r_i$ and the next state $s'_i$ are only conditioned by $s_i$ and $a_i\\sim \\beta(\\cdot | s_i)$, the $g^\\pi(s_i,a_i) = r_i + \\gamma Q(s'_i,\\pi(s'_i))$ are all drawn identically and independently of each other.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7e62e",
   "metadata": {},
   "source": [
    "Despite this, TD(0) with tabular function representation still converges as long as states and actions are tried frequently enough. In some cases of function approximation this is also still true. But we might have found a major issue here in the most general case.\n",
    "\n",
    "One way to (approximately) recover the conditional independence of sampled states $s_i$ is to store a large number of samples $(s_i,a_i,r_i,s'_i)$ in memory, and draw samples uniformly at random from this memory for the TD(0) updates. This idea was first introduced by Lin in his 1992 **[Self-improving reactive agents based on reinforcement learning, planning and teaching](https://link.springer.com/article/10.1007/BF00992699)** article, under the name of *experience replay* (although his derivation was not exactly the same and applied to the Q-learning algorithm which we reserve for another class).\n",
    "\n",
    "The memory of samples is generally called an *experience replay memory* or *experience replay buffer*, since it allows the learning agent to store past experience in memory and recall it (replay it) as many times as necessary to facilitate learning.\n",
    "\n",
    "Drawing uniformly randomly from a replay buffer preserves the stationary distribution which generated the samples and breaks the conditional dependency between successive samples in a trajectory.\n",
    "\n",
    "Combined with the delayed updates introduced earlier, this yields a general, practical algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25231b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**TD(0) with delayed updates and experience replay:**  \n",
    "Given a set of samples $\\left\\{(s_i,a_i,r_i,s'_i)\\right\\}_{i\\in [1,N]}$ all drawn from a fixed behavior distribution, and a *target* function $Q^-$, the gradient update is:\n",
    "$$Q \\leftarrow Q + \\alpha \\sum_{i=1}^N \\left[ r_i + \\gamma Q^-(s'_{i},\\pi(s'_i)) - Q_\\theta(s_i,a_i) \\right] \\nabla_Q Q(s_i,a_i)$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, under the Robbins-Monro conditions, and under repeated substitution of $Q^-$ by $Q_\\theta$ every $C$ gradient updates, this procedure converges to $Q^\\pi$ that minimizes $\\|Q - Q^\\pi \\|_2$.\n",
    "</div>\n",
    "\n",
    "We can write the same update in the case of parametric function approximation.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**TD(0) with delayed updates, experience replay and parametric function approximation:**  \n",
    "Given a set of samples $\\left\\{(s_i,a_i,r_i,s'_i)\\right\\}_{i\\in [1,N]}$ all drawn from a fixed behavior distribution, a *target* function $Q^-$,  and a parametric function approximator $Q_\\theta$, the gradient update is:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{i=1}^N \\left[ r_i + \\gamma Q^-(s'_{i},\\pi(s'_i)) - Q_\\theta(s_i,a_i) \\right] \\nabla_\\theta Q_\\theta(s_i,a_i)$$\n",
    "As long as all state-action pairs $(s,a)$ are sampled infinitely often as $t\\rightarrow\\infty$, under the Robbins-Monro conditions, and under repeated substitution of $Q^-$ by $Q_\\theta$ every $C$ gradient updates, this procedure converges to $\\theta^\\pi$ that minimizes $\\|Q_\\theta - Q^\\pi \\|_2$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90964a86",
   "metadata": {},
   "source": [
    "## Homework: the importance of the behavior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb7e0b",
   "metadata": {},
   "source": [
    "We have written above that TD(0) on $Q$ functions is an off-policy algorithm since it will estimate $Q^\\pi$ whatever the behavior policy is, as long as this policy explores all state-action pairs infinitely often.\n",
    "\n",
    "This is actually only true if we are working with tabular representations of $Q$ functions.\n",
    "\n",
    "Let's return to the gradient descent formulation for a minute. We wrote that the loss was:\n",
    "$$L(Q) = \\int_S \\int_A \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right)\\right]^2 ds da.$$\n",
    "    \n",
    "And we wrote the Monte Carlo estimate of this loss' gradient:\n",
    "$$d = \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i).$$\n",
    "\n",
    "This is actually only correct if the samples $(s_i, a_i)$ are drawn uniformly in $S \\times A$. But in practice, the samples are drawn in $S\\times A$ according to the behavior distribution $\\rho^\\beta(s,a)$.\n",
    "\n",
    "Let us get back to the general formulation of stochastic gradient descent. For this, we will introduce a distribution $\\Gamma$ over $S\\times A\\times \\mathbb{R}$ that describes the joint distribution of state-action pairs $(s,a)$ and long term returns $G^\\pi(s,a)$ under policy $\\pi$. So, given a behavior distribution $\\rho^\\beta(s,a)$, essentially:\n",
    "$$\\Gamma(s,a,g^\\pi) = \\rho^\\beta(s,a) \\mathbb{P}(G^\\pi(s,a)=g^\\pi|s,a)$$\n",
    "\n",
    "Then finding $Q^\\pi$ amounts to solving the minimization problem $\\min_Q L(Q)$ with:\n",
    "\\begin{align*}\n",
    "L(Q) &= \\mathbb{E}_{s,a,g^\\pi \\sim \\Gamma} \\left[ \\left(Q(s,a) - g^\\pi\\right) \\right],\\\\\n",
    "     &= \\int_S \\int_A \\int_\\mathbb{R} \\left[ Q(s,a) - g^\\pi \\right]^2 \\Gamma(s,a,g^\\pi) ds da dg^\\pi.\n",
    "\\end{align*}\n",
    "\n",
    "Then, **if the samples $(s,a,g^\\pi)$ are drawn according to $\\Gamma(s,a,g^\\pi)$**, the Monte Carlo estimate\n",
    "$$d = \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i)\\right] \\nabla_Q Q(s_i,a_i)$$\n",
    "is actually a correct estimate of $\\nabla_Q L(Q)$.\n",
    "\n",
    "What does it mean that $(s,a,g^\\pi)$ are drawn according to $\\Gamma(s,a,g^\\pi)$? It means state-action pairs are drawn according to $\\rho^\\beta(s,a)$ and the $g^\\pi(s_i,a_i)$ follow the distribution of $G^\\pi(s,a)$\n",
    "\n",
    "Otherwise, this Monte Carlo estimate it is a biased estimator that minimizes another loss function; for example one defined by another behavior distribution $\\rho^\\beta(s,a)$.\n",
    "\n",
    "So when we are summing elements drawn from the replay memory, or when we are using single samples drawn from the interaction with the MDP, we are actually minimizing the loss function defined specifically by $\\rho^\\beta(s,a)$. And in the end, for another behavior distribution, the resulting minimizer $Q$ of the loss might be different from the one obtained by using samples collected with $\\rho^\\beta$.\n",
    "\n",
    "So, does that mean TD(0) on $Q$ functions is not really off-policy?\n",
    "\n",
    "In the case of tabular representations, the fact that the gradients $\\nabla_Q Q(s_i,a_i)$ are actually indicator functions of $(s_i, a_i)$ limits the impact of having different distributions on $S\\times A$. In this case, the minimizers of $L(Q)$ are actually all the same across behavior distributions, as long as the behavior distribution's support spans fully $S\\times A$.\n",
    "\n",
    "However, this nice property is lost in the general case, in particular in the case of function approximation $Q_\\theta$. This has motivated the introduction of the **[Gradient Temporal Difference](https://proceedings.neurips.cc/paper/2008/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html)** family of algorithms. This topic is beyond the scope of this class but the interested reader is encouraged to look at **[H. R. Maei's PhD thesis](https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950acc6",
   "metadata": {},
   "source": [
    "## Homework: Monte Carlo evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073059c",
   "metadata": {},
   "source": [
    "When evaluating a given policy, we directly *bootstrapped* the estimator of $V^\\pi(s_t,a_t)$, by writing $g_t^\\pi = r_t + \\gamma V_t(s_{t+1})$.\n",
    "\n",
    "But an immediate way to estimate $V^\\pi(s)$ is to take the empirical average of a series if realizations of the $\\sum\\limits_{t = 0}^\\infty \\gamma^t R_t$ random variable.\n",
    "\n",
    "In plain words: simulate $\\pi$ from $s$ a certain number of times to obtain trajectories, observe and accumulate the rewards along each trajectory, take the empirical average over all trajectories.\n",
    "\n",
    "That is precisely what we did in section 5's homework to estimate the value of the starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39567f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/RL1_exercise1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054fb6e",
   "metadata": {},
   "source": [
    "Now can we generalize this to learning the value $V^\\pi$ in all states?\n",
    "\n",
    "Let us start with a fully **offline Monte-Carlo** algorithm. The idea is simple: start from $s$, run the policy until termination (or for a long number of steps), repeat for a number of episodes, then update the value of all encountered states. This requires to store in memory full episodes (it also requires that the episodes be finite-length).\n",
    "\n",
    "But we can immediately do better with an **online Monte-Carlo** method. It is almost the same idea: start from $s$, run the policy until termination (or for a long number of steps) then update the value of all encountered states before restarting an episode.  \n",
    "\n",
    "Let $(s_0, r_0, s_1, \\ldots, s_T)$ be the sequence of transitions of such an episode.  \n",
    "For the sake of clarity we will slim down our notations: $g^\\pi(s_t)$ becomes $g_t$.  \n",
    "Then, this sequence provides a sample $g_t$ of $G^\\pi(s_t)$ for all $s_t$ visited during the simulations. \n",
    "<div class=\"alert alert-success\"><b>Monte Carlo return:</b>\n",
    "$$g_t = \\sum_{i>t} \\gamma^{i-t} r_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e00e3e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Consider the policy that always moves right in FrozenLake and implement an online Monte-Carlo estimatior for $g_t$, that you will use in a Stochastic Approximation method to obtain $V^\\pi$.  \n",
    "We decide we can tolerate a 0.001 error on $V^\\pi$ so, to keep things simple, we set a constant $\\alpha=0.001$.  \n",
    "Take $\\gamma = 0.9$. Run 100000 episodes of maximum 1000 time steps each.  \n",
    "After each episode, compare the value function obtained with that computed with the model-based approaches of previous sections (use the cell below to recall `V_pi0` from section 6's exercises).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2528b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/RL2_exercise5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13271628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL3_exercise1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada2a40",
   "metadata": {},
   "source": [
    "So online Monte-Carlo allows us to update $V^\\pi$ episode after episode. Some values are better estimated than others depending on how often the corresponding state was visited.\n",
    "\n",
    "Monte-Carlo estimation has some flaws nonetheless. It still requires to store one full episode in memory before $V$ is updated. Also, one rare value for $r_t$ affects directly all the value of the states encountered before $s_t$. So we can question the robustness of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a652ef",
   "metadata": {},
   "source": [
    "## Homework: TD(lambda)\n",
    "\n",
    "With Monte Carlo (MC) and TD(0), we have two methods with different features:\n",
    "- TD(0): 1-sample update with bootstrapping\n",
    "- MC: $\\infty$-sample update with no bootstrapping\n",
    "\n",
    "What's inbetween?\n",
    "- inbetween: $n$-sample update with bootstrapping\n",
    "\n",
    "We define the **$n$-step target** or **$n$-step return** $G^{(n)}_t$ from state $s_t$ as the random variable:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|l}\n",
    "G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots & \\textrm{MC}\\\\\n",
    "G^{(1)}_t = R_t + \\gamma V_t(S_{t+1}) & 1\\textrm{-step TD = TD(0)}\\\\\n",
    "G^{(2)}_t = R_t + \\gamma R_{t+1} + \\gamma^2 V_t(S_{t+2}) & 2\\textrm{-step TD}\\\\\n",
    "G^{(n)}_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\ldots + \\gamma^n V_t(S_{t+n}) & n\\textrm{-step TD}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "And we define the **$n$-step TD update** as:\n",
    "<div class=\"alert alert-success\"><b>$n$-step TD update:<b>\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ G^{(n)}_t - V(s_t) \\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297f94d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Suppose that the immediate reward $R$ has a constant variance $\\sigma^2$ and that for all states $s$ the estimator $V(s)$ of $V^\\pi(s)$ has bias $\\epsilon$.  \n",
    "What is the variance of $G_t^{(n)}$?  \n",
    "What is the bias of $\\mathbb{E}\\left(G_t^{(n)}(s)\\right)$ as an estimator of $V^\\pi(s)$?  \n",
    "Comment on the impact of choosing a certain value for $n$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037c252",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Reminder: $var(X+Y)=var(X)+var(Y)$ and $var(aX)=a^2 var(X)$.  \n",
    "Consequently:\n",
    "\\begin{align*}\n",
    "    var(G_t^{(n)}) &= \\sum_{i=0}^{n-1} \\gamma^{2i} \\sigma^2 + \\gamma^{2n} var(V_t(S_{t+n}))\\\\\n",
    "     &= \\frac{1-\\gamma^{2n}}{1-\\gamma^2}\\sigma^2 + \\gamma^{2n} var(V_t(S_{t+n}))\n",
    "\\end{align*}\n",
    "    \n",
    "The variance grows with $n$, both because $1-\\gamma^n$ grows with $n$ and because $S_{t+n}$ has larger variance as $n$ grows.\n",
    "    \n",
    "On the bias side:\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}\\left(G_t^{(n)}(s)\\right) - V^\\pi(s) &= \\mathbb{E}\\left(G_t^{(n)}(s)\\right) - \\mathbb{E}\\left(\\sum_{i=0}^\\infty \\gamma^t R_t\\right)\\\\\n",
    "    &=\\mathbb{E}\\left(\\sum_{i=0}^{n-1} \\gamma^i R_{t+i} + \\gamma^n V_t(S_{t+n})\\right)  - \\mathbb{E}\\left(\\sum_{i=0}^\\infty \\gamma^t R_t\\right)\\\\\n",
    "    &=\\gamma^n \\left[ \\mathbb{E}\\left(V_t(S_{t+n})\\right) - \\mathbb{E}\\left(\\sum_{i=n}^\\infty \\gamma^t R_t \\right) \\right]\\\\\n",
    "    &=\\gamma^n \\left[ \\mathbb{E}\\left(V_t(S_{t+n})\\right) - V_t(S_{t+n}) \\right]\\\\\n",
    "    &=\\gamma^n \\epsilon\n",
    "\\end{align*}\n",
    "    \n",
    "So the bias decreases with $n$. This makes sense since $V_t$'s importance is weighted by $\\gamma^n$.\n",
    "    \n",
    "Consequently, choosing a value for $n$ is making a bias-variance tradeoff. Small $n$ means small variance an large bias, large $n$ means large variance and small bias. Thus, choosing an intermediate value has in interest in accelerating the convergence of TD algorithms.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503df30b",
   "metadata": {},
   "source": [
    "So MC corresponds to an $\\infty$-step TD update.  \n",
    "    \n",
    "The $n$-step TD update algorithm converges to the true $V^\\pi$ just as TD(0) or MC. It requires to wait for $n$ time steps before performing an update.\n",
    "\n",
    "Remark: for finite-length  episodes of length $T$, all $n$-step returns for $n>T-t$ are equal to the Monte Carlo return $G_t$.\n",
    "\n",
    "So $n$-step TD updates bridge a gap between MC and TD(0). But it's not quite satisfying yet because we never really know what value of $n$ is appropriate to speed up convergence for a given problem. An interesting property is that we can mix $n$ and $m$-step returns together. Consider $G^{mix}_t = \\frac{1}{3} G^{(2)}_t + \\frac{2}{3} G^{(4)}_t$.\n",
    "Then the update $V(s_t) \\leftarrow V(s_t) + \\alpha \\left[G^{mix}_t - V(s_t)\\right]$ still converges to $V^\\pi$. More generally, convex sums of $n$-step returns yield update procedures that still converge to $V^\\pi$.\n",
    "\n",
    "Now, take $\\lambda\\in [0,1]$ and consider the $\\lambda$-return $G^\\lambda_t$:\n",
    "<div class=\"alert alert-success\"><b>$\\lambda$-return:</b>\n",
    "$$G^\\lambda_t = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^\\infty \\lambda^{n-1}G_t^{(n)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943f4e4",
   "metadata": {},
   "source": [
    "The $\\lambda$-return is the mixing of *all* $n$-step returns, with weights $(1-\\lambda) \\lambda^{n-1}$. So, an agent performing a $\\lambda$-return update looks one step in the future and uses that step to update $V(s)$ with weight $(1-\\lambda)$, then looks 2 steps into the future and updates $V(s)$ with a weight $\\lambda (1-\\lambda)$ and so on. The illustrative figure below is an excerpt from **Reinforcement Learning: an introduction** by Sutton and Barto.\n",
    "\n",
    "<img src=\"img/TD_lambda_forward.png\"></img>\n",
    "\n",
    "To get a better understanding of the $\\lambda$-return and to set ideas, let us consider a finite length episode $(s_t, r_t, s_{t+1}, \\ldots, s_T)$. Since the episode ends after $T$, we have $\\forall k>0, \\ G^{(T-t+k)}_t = G_t$. Thus, we can split the $\\lambda$-return sum in two:\n",
    "\n",
    "\\begin{align*}\n",
    "G^\\lambda_t & = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\sum\\limits_{n=T-t}^{\\infty} \\lambda^{n-1}G_t^{(n)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\lambda^{T-t-1} \\sum\\limits_{n=T-t}^{\\infty} \\lambda^{n-T+t} G_t^{(n)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\left(1-\\lambda\\right) \\lambda^{T-t-1} \\sum\\limits_{k=0}^{\\infty} \\lambda^{k} G_t^{(T-t+k)}\\\\\n",
    "& = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t-1} G_t\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So we have $G^\\lambda_t = \\left(1-\\lambda\\right) \\sum\\limits_{n=1}^{T-t-1} \\lambda^{n-1}G_t^{(n)} + \\lambda^{T-t-1} G_t$.\n",
    "- When $\\lambda = 0$, it is a $TD(0)$ update (hence the \"0\" in TD(0)).\n",
    "- When $\\lambda = 1$, it is a MC update.\n",
    "So we can define the **$\\lambda$-return algorithm** that generalizes on TD(0) and MC:\n",
    "<div class=\"alert alert-success\"><b>$\\lambda$-return algorithm:</b>\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\left[G^{\\lambda}_t - V(s_t)\\right] $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4d8ca",
   "metadata": {},
   "source": [
    "That's all very nice and we have replaced the choice of $n$ by the choice of $\\lambda$, which seems less sensitive. But, still, we don't know how to compute those $n$-step returns, and the $\\lambda$-return, without running $n$-step episodes (and thus infinite episodes for the $\\lambda$-return in the general case).\n",
    "\n",
    "This is where we need to flip the little man in the drawing above to make him look backwards in time. When an agent transitions from $s$ to $s'$ and obtains reward $r$, it can compute the $1$-step return for $s$ and perform the corresponding $1$-step TD update. Then, as it transitions from $s'$ to $s''$ and observes $r'$ it can perform the $1$-step TD update in $s'$, but also the $2$-step TD update in $s$! An so on for future transitions. So, incrementally, as time unrolls, the agent will include the $n$-step updates in the $\\lambda$-return of $s$ as they become available. In the limit, when $t\\rightarrow\\infty$, the $\\lambda$-return in every state will be complete and the agent will have completed a $\\lambda$-return algorithm. This figure below (excerpt from **Reinforcement Learning: an introduction** by Sutton and Barto) illustrates this *backward-view* on TD updates.\n",
    "\n",
    "<img src=\"img/TD_lambda_backward.png\"></img>\n",
    "\n",
    "This seems to imply that we need to remember the states we went through, which is quite the same as remembering full episodes for MC updates. But since we want to update a state seen $n$ steps ago with a weight $\\lambda^n (1-\\lambda)$, we just need to remember, for each state, the last time we visited it (and we can forget about the trajectory linking states together). This way, we store $|S|$ values at all time, instead of an increasingly long sequence of transitions. In order to do this, we introduce the notion of **eligibility trace**:\n",
    "<div class=\"alert alert-success\"><b>Eligibility trace of state $s$:</b>\n",
    "$$e_t(s) = \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e_{t-1}(s) & \\textrm{if }s\\neq s_t\\\\\n",
    "1 & \\textrm{if }s = s_t\n",
    "\\end{array}\\right.$$\n",
    "</div>\n",
    "\n",
    "Initially, all states have an eligibility trace of zero. The eligibility trace of an unvisited state decays exponentially. So $e_t(s)$ measures how old the last visit of $s$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a61b9e",
   "metadata": {},
   "source": [
    "Note that two alternative definitions of eligibility traces prevail:\n",
    "<ul>\n",
    "    <li> \"<b>replacing traces</b>\": $e_t(s) = 1\\textrm{ if }s = s_t$\n",
    "    <li> \"<b>accumulating traces</b>\": $e_t(s) = e_{t-1}(s) + 1\\textrm{ if }s = s_t$\n",
    "</ul>\n",
    "Often (not always), replacing traces are used in practice.<br>\n",
    "<br>\n",
    "And finally we can define the TD($\\lambda$) algorithm:\n",
    "<div class=\"alert alert-success\"><b>TD($\\lambda$) algorithm:</b><br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> Temporal difference $\\delta = r_t+\\gamma V(s_t') - V(s_t)$.\n",
    "<li> Update eligibility traces for all states<br>\n",
    "$e(s) \\leftarrow \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e(s) & \\textrm{if } s\\neq s_t\\\\\n",
    "1 & \\textrm{if } s=s_t\n",
    "\\end{array}\\right.$\n",
    "<li> Update all state's values $V(s) \\leftarrow V(s) + \\alpha e(s) \\delta$\n",
    "</ol>\n",
    "Initially, $e(s)=0$.\n",
    "</div>\n",
    "\n",
    "Properties and remarks:\n",
    "- Earlier states are given $e(s)$ *credit* for the TD error $\\delta$\n",
    "- If the environment contains terminal states, then $e$ should be reset to zero whenever a new trajectory begins.\n",
    "- If $\\lambda=0$, $e(s)=0$ except in $s_t$ $\\Rightarrow$ standard TD(0)\n",
    "- For $0<\\lambda<1$, $e(s)$ indicates a distance $s \\leftrightarrow s_t$ is in the episode.\n",
    "- If $\\lambda=1$, $e(s)=\\gamma^\\tau$ where $\\tau=$ duration since last visit to $s_t$ $\\Rightarrow$ MC method<br>\n",
    "TD(1) implements Monte Carlo estimation on non-episodic problems!<br>\n",
    "TD(1) learns incrementally for the same result as MC\n",
    "- **TD($\\lambda$) is equivalent to the $\\lambda$-return algorithm.**\n",
    "- The value of $\\lambda$ can even be changed during the algorithm without impacting convergence.\n",
    "- TD($\\lambda$) is an on-policy algorithm: samples must be collected following the policy under evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76386c0c",
   "metadata": {},
   "source": [
    "Note that TD($\\lambda$) is already a batch update (it already updates all state values) but not in the sense of SGD batches.\n",
    "\n",
    "However, in the presentation given above, since the eligibility trace $e(s)$ is defined state by state, the formulation of TD($\\lambda$) is limited to discrete state spaces and tabular function representations. The section on function approximation further down will provide an extension of TD($\\lambda$) to linear function approximation.\n",
    "\n",
    "The extension of TD($\\lambda$) to the off-policy setting has been undertaken in the more general work about **[Gradient Temporal Differences](https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a)** quoted earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af839903",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Implement a TD($\\lambda$) algorithm to estimate $V^\\pi$ fo the policy that always goes right. As before, take a constant $\\alpha=0.001$, $\\gamma=0.9$ and $\\lambda=0.5$. Run the algorithm for 2000000 steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d174de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL3_exercise4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f837ba",
   "metadata": {},
   "source": [
    "## Homework: value function approximation\n",
    "\n",
    "Often, $S$ is not finite (or is just too large to be enumerated). Consequently, $\\mathcal{F}(S,\\mathbb{R})$ has infinite (or just too large) dimension. Thus, tabular representations of $V$ are not possible and one needs to turn to function representations $V_\\theta$ or $Q_\\theta$ with parameters $\\theta$. In this section, we provide a very short introduction to approximation methods for $V$ and $Q$.\n",
    "\n",
    "The FrozenLake example is a toy problem with very few states (moreover discrete). It does not lend itself to a convincing demonstration of value function approximation. We shall remain at the theoretical level for the following considerations and reserve practice for later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374889e3",
   "metadata": {},
   "source": [
    "**Linear value function approximation**\n",
    "\n",
    "Suppose we write $V$ as a linear model:\n",
    "$$V(s) = \\theta^T \\varphi(s) = \\sum_{i=1}^K \\theta_i \\varphi_i(s)$$\n",
    "\n",
    "We wish to approximate $V(s)$ as a linear combination of features $\\varphi(s)=\\left(\\varphi_i(s)\\right)_{i\\in[1,K]}$. This way, $V$ lives in the $K$-dimensional function space $span(\\varphi)$. We have plenty of families of functions that we can rely on and the user's expertise plays a big role in choosing a proper **functional basis**. Generally speaking, we would expect the following properties from a good basis:\n",
    "- the target $V^\\pi$ can be closely approximated by its projection on $\\varphi$\n",
    "- given an initial $V_0 \\in span(\\varphi)$ and the recurrence relation $V_{n+1} = \\Pi_\\varphi (T^\\pi V_n)$ (where $\\Pi_\\varphi$ is the projection operator on $span(\\varphi)$), $V_n$ should be a \"close enough\" approximation of $T^\\pi V_n$. This property is illustrated by the figure below with $Q$ instead of $V$ - excerpt from **[Least-Squares Policy Iteration](https://www.jmlr.org/papers/v4/lagoudakis03a.html)** by M. G. Lagoudakis and R. Parr (2003).\n",
    "- $\\varphi$ should form a basis (that is $\\varphi_i \\bot \\varphi_j$)\n",
    "\n",
    "<img src=\"img/projection.png\" style=\"width: 600px;\"></img>\n",
    "\n",
    "If $\\sum_{i=1}^K \\varphi_i(s) = 1$, then $V_\\theta$ is called an *averager*. Averagers are known to be well-behaved for iterative function approximation. Otherwise, other non-averager families of functions are commonly used:\n",
    "\n",
    "- $\\cos$, $\\sin$ over state variables (mimics the Fourier transform, extends to wavelet bases)\n",
    "- polynomials of the state variables (mimics the Taylor expansion)\n",
    "- radial basis functions of the state variables (performs local approximation, extends to kernel smoothing).\n",
    "- among averagers, piecewise constant local functions $\\varphi_i(s) \\in \\{0;1\\}$ group *neighborhoods* in the state space together (note the similarity with tree-based regressors).\n",
    "\n",
    "A very straightforward way of building feature sets is to define features depending on a single state variable and then using the tensor product in order to obtain all possible combinations of sigle-variable features. More formally and more generally, suppose $S \\subset S_1\\times \\ldots \\times S_k$ and suppose $\\varphi^{(i)}$ defines $d_i$ features over $S_k$; then the tensor product $\\varphi^{(1)} \\otimes \\ldots \\otimes \\varphi^{(k)}$ yields $d=d_1\\ldots d_k$ feature functions on $S$. But there is a catch, the number of these resulting features grows exponentially with $k$ and so does the dimension of the value function's search space $span(\\varphi)$: that is the **curse of dimensionality** that makes searching for a value function exponentially more difficult as the state space dimension grows.\n",
    "\n",
    "Additionnaly, there is **no guarantee** that, for a given $V_n \\in span(\\varphi)$, $T^\\pi V_n$ actually lives in $span(\\varphi)$.\n",
    "\n",
    "But on the bright side, given an initial state $s_0$, the actual reachable space $S'$ given $\\pi$ might be much smaller than $S$. So, in practice, we just need to obtain a good approximation of $V$ on the subspace $S'$.\n",
    "\n",
    "Anyway, to conclude this short paragraph on feature engineering:\n",
    "- good feature engineering in RL is even more crucial than in supervised learning.\n",
    "- it can be very problem-dependent.\n",
    "- good function approximators (generally non-parametric to avoid the fixed $span(\\varphi)$) are of crucial importance.\n",
    "We will discuss non-linear and non-parametric function approximation a bit further in the notebook.\n",
    "\n",
    "Linear function approximation has played a major role in the RL literature, in particular for temporal differences methods. The **[Policy Evaluation with Temporal Differences](https://www.jmlr.org/papers/v15/dann14a.html)** survey by C. Dann et al. (2014) provides a great overview of this literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139da50",
   "metadata": {},
   "source": [
    "**The tabular case is just a specific case of linear approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbd0d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "If the sentence above is true, what is the set of basis functions corresponding to a tabular representation of $V$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2ac21",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "In the discrete state space case, consider the averager defined as:\n",
    "$$\\varphi_i(s) = \\left\\{\\begin{array}{ll}1 & \\textrm{if }s=s_i\\\\ 0 & \\textrm{otherwise}\\end{array}\\right.$$\n",
    "Feature function $\\varphi_i$ is the indicator function of state $s_i$. Therefore, we have $|S|$ feature functions. So when we write $V(s) = \\sum_{i=1}^{|S|} \\theta_i \\varphi_i(s)$, we actually have $V(s_i) = \\theta_i$. Therefore the tabular representation of $V$ is equivalent to a linear model with the $\\varphi_i$ feature functions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024afa94",
   "metadata": {},
   "source": [
    "Based on the previous exercise, let us rewrite TD($\\lambda$) as a linear model update (we take the accumulating traces version; the replacing traces case is equivalent). We had previously:<br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> Temporal difference $\\delta = r_t+\\gamma V(s_t') - V(s_t)$.\n",
    "<li> Update eligibility traces for all states<br>\n",
    "$e(s) \\leftarrow \\left\\{\\begin{array}{ll}\n",
    "\\gamma \\lambda e(s) & \\textrm{if } s\\neq s_t\\\\\n",
    "1 + \\gamma \\lambda e(s)& \\textrm{if } s=s_t\n",
    "\\end{array}\\right.$\n",
    "<li> Update all state's values $V(s) \\leftarrow V(s) + \\alpha e(s) \\delta$\n",
    "</ol>\n",
    "Initially, $e(s)=0$.\n",
    "\n",
    "The temporal difference can be rewritten $\\delta = r_t+\\gamma\\theta^T \\varphi(s_t') - \\theta^T \\varphi(s_t)$.\n",
    "\n",
    "The eligibility trace update can be rewritten $e \\leftarrow \\varphi(s) + \\gamma \\lambda e$.\n",
    "\n",
    "Similarly the value update can be rewritten $\\theta \\leftarrow \\theta + \\alpha e \\delta$.\n",
    "\n",
    "Remark:  \n",
    "Recall the discussion on the importance of the behavior distribution? We concluded that tabular representations were a specific case where TD(0) is truly off-policy because the influence of a sample was limited to the value of $Q$ in the corresponding $(s,a)$ pair. This discussion can be generalized for averagers (although it remains an approximation): such local models are well-behaved to suffer less from the shift in behavior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdac818",
   "metadata": {},
   "source": [
    "**TD($\\lambda$) as a linear approximation update**\n",
    "\n",
    "We generalize the previous result to the general linear model case:\n",
    "<div class=\"alert alert-success\"><b>TD($\\lambda$) with linear function approximation:</b><br>\n",
    "With $V(s) = \\sum_{i=1}^K \\theta_i \\varphi_i(s)$, $e \\in \\mathbb{R}^K$.<br>\n",
    "Initially, $e=0$.<br>\n",
    "Given a new sample $(s_t,a_t,r_t,s_t')$.\n",
    "<ol>\n",
    "<li> Temporal difference $\\delta = r_t+\\gamma\\theta^T \\varphi(s_t') - \\theta^T \\varphi(s_t)$.\n",
    "<li> Update eligibility traces for all states $e \\leftarrow \\varphi(s) + \\gamma \\lambda e$\n",
    "<li> Update value function $\\theta \\leftarrow \\theta + \\alpha e \\delta$\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "Note that we have provided the results above without proof. We will admit them and refer the reader to RL textbooks for a rigorous justification.\n",
    "\n",
    "Further reading on TD($\\lambda$) with linear function approximation: \n",
    "**[True Online TD($\\lambda$)](http://proceedings.mlr.press/v32/seijen14.html)** by H. Van Seijen and R. Sutton (2014).  \n",
    "An unpublished negative result that somehow follows from this article is also that in the general case it is not possible to have a TD($\\lambda$) algorithm performing on non-linear function approximation and being equivalent to the $\\lambda$-return algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1c57c",
   "metadata": {},
   "source": [
    "**Non-parametric models**\n",
    "\n",
    "Non-parametric models generally refer to function approximators that do not rely on an a-priori fixed finite-dimensional search space and allows the representation space to evolve as needed. Among those non-parametrics models, one can count:\n",
    "- linear approximations that incrementally enrich the functional basis (e.g. **[this article](https://dl.acm.org/doi/abs/10.1145/1390156.1390251)**).\n",
    "- general supervised learning methods: SVMs, k-nearest neighbours (kernel smoothing methods), Gaussian Processes, tree-based methods, neural networks, etc.\n",
    "  In this second category, it is generally useful to distinguish between\n",
    "  - methods that explicitly minimize the L2 loss defined earlier (e.g. neural networks),\n",
    "  - methods that minimize some other loss (e.g. random forests) and provide alternate (better or worse guarantees).\n",
    "\n",
    "One quickly realizes that the frontier between parametric and non-parametric models is blur. In the general case of a $s \\mapsto V(s)$ function approximator, the general idea is to feed this approximator with samples of the form $(s, r+\\gamma V(s'))$. But beware: most of the nice results are generally lost when one leaves the realm of linear function approximation. More precisely, when one combines **function approximation**, **off-policy learning** and **bootstrapping** in a temporal difference method, all results are generally lost. This has been studied as the **[Deadly triad of Reinforcement Learning](https://arxiv.org/abs/1812.02648v1)** (H. Van Hasselt et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544575ae",
   "metadata": {},
   "source": [
    "## Homework: Generalized Policy Iteration and Actor-Critic architectures\n",
    "\n",
    "Remember how we went from Policy Iteration (figure below) to Asynchronous Policy Iteration?\n",
    "<img src=\"img/policyiteration.png\"></img>\n",
    "\n",
    "Now recall the principle of Asynchronous Policy Iteration. If we consider the two elementary operations:\n",
    "- Bellman backup on $Q$: $Q(s,a) \\leftarrow r(s,a) + \\gamma \\sum\\limits_{s'} p(s'|s,a) Q(s',\\pi(s'))$\n",
    "- Bellman backup on $\\pi$: $\\pi(s) \\leftarrow \\arg\\max_{a} Q(s,a)$\n",
    "\n",
    "Then, as long as every state and every action is visited infinitely often for Bellman backups on $Q$ or $\\pi$, the sequences of $Q_n$ and $\\pi_n$ converge to $Q^*$ and $\\pi^*$.\n",
    "\n",
    "Value Iteration: in each state, one update of $Q$ and one improvement of $\\pi$.<br>\n",
    "Policy Iteration: update $Q$ in all states until convergence, then update $\\pi$ in all states.\n",
    "\n",
    "**Generalized Policy Iteration** is the case where one has two interacting processes: policy evaluation and policy improvement, directly from samples (not from the model anymore). If these processes converge to their respective targets, then Generalized Policy Iteration converges to $Q^*$ and $\\pi^*$. Model-free policy evaluation can take many forms: indirect RL, Monte Carlo evaluations, TD methods...\n",
    "\n",
    "This leads to the definition of the general **actor-critic architectures** (excerpt from [**Algorithms for Reinforcement Learning**](https://sites.ualberta.ca/~szepesva/rlbook.html) by C. Szepesvari):\n",
    "\n",
    "<img src=\"img/actor-critic.png\"  style=\"width: 500px;\"></img>\n",
    "\n",
    "In such architectures, an *actor* chooses an action based on the current state and the information provided by the *critic*, while the *critic* constantly aims at learning relevant things in order to help the *actor* decide (value functions for example, or an approximate model).\n",
    "\n",
    "Almost all Reinforcement Learning algorithms fall into an actor-critic architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752c0ff",
   "metadata": {},
   "source": [
    "## Homework: SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ffa3a5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Suppose we modify TD(0) on $Q$-functions so that, instead of picking a random action at each time step, we pick the greedy action with respect to $Q$. What is the risk in doing this?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3c779",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "A deterministic, greedy policy will not try all actions in all states infinitely often. So we can't guarantee convergence, neither of $Q$ to any policy's $Q^\\pi$ (because if some states are not visited no updates will take place for them), nor of the $Q$-greedy policy to $\\pi^*$. The conditions for the convergence of Generalized Policy Iteration are just not met.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbaee2d",
   "metadata": {},
   "source": [
    "To solve this issue we introduce the notion of **Greedy in the limit of infinite exploration** (GLIE) actor:  \n",
    "\n",
    "<div class=\"alert alert-success\"><b>Greedy in the limit of infinite exploration</b> (GLIE) actor:<br>\n",
    "A GLIE actor guarantees that:<br>\n",
    "- All state-action pairs $(s,a)$ are visited infinitely often for updates of $Q$, as $t\\rightarrow\\infty$:\n",
    "$$\\lim_{t\\rightarrow\\infty} count_t(s,a) = \\infty$$\n",
    "- As $t\\rightarrow\\infty$ the actor becomes $Q$-greedy, that is:\n",
    "$$\\lim_{t\\rightarrow\\infty} \\pi_t(a|s) = \\mathbb{1}\\left(a = \\arg\\max_{\\hat{a}} Q(s,\\hat{a})\\right)$$\n",
    "</div>\n",
    "\n",
    "An example of GLIE actor is the so-called $\\epsilon$-greedy exploration strategy (that we introduced with Q Learning) that uniformly picks a non-greedy action with probability $\\epsilon$:\n",
    "$$\\pi_t(a|s) = \\left\\{\\begin{array}{ll}1-\\epsilon_t & \\textrm{if }a=\\arg\\max_{\\hat{a}} Q(s,\\hat{a})\\\\\n",
    "\\frac{\\epsilon_t}{|A|-1} & \\textrm{otherwise} \\end{array}\\right.$$\n",
    "With a parameter $\\epsilon_t>0$ that goes to zero as $t$ tends to $\\infty$, one obtains a GLIE actor.\n",
    "\n",
    "GLIE actors (or policies) enforce the limits of the **exploration vs. exploitation tradeoff**. As long as actors respect the GLIE properties, actor-critic architectures fall within the convergence properties of Generalized Policy Iteration.\n",
    "\n",
    "With this last definition, we have all the ingredients to define algorithms that evolve to an optimal behaviour.\n",
    "\n",
    "**A remark**\n",
    "\n",
    "You might have noticed that the totally random policy we applied in the TD(0) update was a very naive choice. Even if all states are reachable from anywhere in the state space (ergodicity property), they might not all be visited with the same frequency and therefore the convergence to $Q^\\pi$ might be delayed because of this uniform exploration strategy.  \n",
    "\n",
    "The same remark applies to $\\epsilon$-greedy exploration strategies that often start with $\\epsilon=1$ and let it slowly decrease towards zero. These strategies don't account for the actual *visitation frequencies* of state-action pairs. In some states it might be good to keep a strong exploration probability because they actually have been seldom visited, while in other states, a faster decrease is desirable. This links also to the question of *value propagation* that was underpinned by the asynchronous value iteration remarks in section 6.  \n",
    "\n",
    "The point here is to notice that $\\epsilon$-greedy is a simple, very naive exploration strategy that fits within the GLIE requirements but that much better exploration policies are possible by taking the current state into account (contextual exploration) or by using the values of the temporal difference (prioritized experience replay) and the $Q$ estimate (Boltzmann policies, $E^3$ or $R_{max}$ strategies) for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24962345",
   "metadata": {},
   "source": [
    "**SARSA**\n",
    "\n",
    "The key idea behind the SARSA algorithm is to build a critic that constantly tries to evaluate the value $Q$ of the actor's policy $\\pi$, and an actor that tends to be greedy with respect to this critic. The algorithm is written:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**SARSA**  \n",
    "In $s$, choose (*actor*) $a$ using $Q$, then repeat:\n",
    "<ol>\n",
    "<li> Observe $r$, $s'$\n",
    "<li> Choose $a'$ (<i>GLIE actor</i>) using $Q$\n",
    "<li> Temporal difference: $\\delta=r+\\gamma Q(s',a') - Q(s,a)$\n",
    "<li> Update $Q$: $Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta$\n",
    "<li> $s\\leftarrow s'$, $a\\leftarrow a'$\n",
    "</ol>\n",
    "SARSA converges if the actor is GLIE and if $\\alpha$ respects the Robbins-Monro conditions.\n",
    "</div>\n",
    "\n",
    "It is important to note that SARSA is an **on-policy** critic: it constantly evaluates the current $\\pi$... that constantly shifts towards $\\pi^*$ by being $Q$-greedy.\n",
    "\n",
    "The name SARSA comes from the usage of an augmented sample $(s,a,r,s',a')$.<br>\n",
    "<br>\n",
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Let's implement an $\\epsilon$-greedy SARSA on the FrozenLake problem.<br>\n",
    "For the decrease of $\\epsilon$ we can opt for a division by 2 every million steps.<br>\n",
    "Keep track of the state-action visitation count, as in the Q-learning example.<br>\n",
    "You can compare with $Q^*$ and $\\pi^*$ obtained during the model-based class.<br>\n",
    "The utility functions below are here to make the task easier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-based policy optimization and a few utility functions\n",
    "\n",
    "def value_iteration(V,epsilon,max_iter):\n",
    "    W = np.copy(V)\n",
    "    residuals = np.zeros((max_iter))\n",
    "    for i in range(max_iter):\n",
    "        for s in range(env.observation_space.n):\n",
    "            Q = np.zeros((env.action_space.n))\n",
    "            for a in range(env.action_space.n):\n",
    "                outcomes = env.unwrapped.P[s][a]\n",
    "                for o in outcomes:\n",
    "                    p  = o[0]\n",
    "                    s2 = o[1]\n",
    "                    r  = o[2]\n",
    "                    Q[a] += p*(r+gamma*V[s2])\n",
    "            W[s] = np.max(Q)\n",
    "            #print(W[s])\n",
    "        residuals[i] = np.max(np.abs(W-V))\n",
    "        #print(\"abs\", np.abs(W-V))\n",
    "        np.copyto(V,W)\n",
    "        if residuals[i]<epsilon:\n",
    "            residuals = residuals[:i+1]\n",
    "            break\n",
    "    return V, residuals\n",
    "\n",
    "def Q_from_V(V):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[s][a]\n",
    "            for o in outcomes:\n",
    "                p  = o[0]\n",
    "                s2 = o[1]\n",
    "                r  = o[2]\n",
    "                Q[s,a] += p*(r+gamma*V[s2])\n",
    "    return Q\n",
    "\n",
    "def greedyQpolicy(Q):\n",
    "    pi = np.zeros((env.observation_space.n),dtype=np.int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        pi[s] = np.argmax(Q[s,:])\n",
    "    return pi\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s%env.unwrapped.ncol\n",
    "    row = int((s-col)/env.unwrapped.ncol)\n",
    "    return row,col\n",
    "\n",
    "def print_policy(pi):\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return\n",
    "\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(Vstar)\n",
    "print(actions)\n",
    "print(Vstar)\n",
    "print(Qstar)\n",
    "pi_star = greedyQpolicy(Qstar)\n",
    "print_policy(pi_star)\n",
    "env.render();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL4_exercise4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ee9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_map = np.zeros((env.unwrapped.nrow, env.unwrapped.ncol, env.action_space.n))\n",
    "for a in range(env.action_space.n):\n",
    "    for x in range(env.observation_space.n):\n",
    "        row,col = to_row_col(x)\n",
    "        count_map[row, col, a] = count[x,a]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4)\n",
    "for a in range(env.action_space.n):\n",
    "    name = \"a = \" + actions[a]\n",
    "    axs[a].set_title(name)\n",
    "    axs[a].imshow(np.log(count_map[:,:,a]+1), interpolation='nearest')\n",
    "    #print(\"a=\", a, \":\", sep='')\n",
    "    #print(count_map[:,:,a])\n",
    "plt.show()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb6583",
   "metadata": {},
   "source": [
    "The same remark as for TD(0) holds: with a better exploration strategy, the convergence to Qstar could be much more efficient. We can see that some states have been visited (and updated) way more often than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4620501",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercices (open questions, no solutions provided):</b><br>\n",
    "<ul>\n",
    "<li><b>Context-dependent exploration</b><br>\n",
    "Can you implement an $(s,a)$-dependent $\\epsilon$-greedy exploration strategy (by using the `count` table introduced earlier for instance)?\n",
    "<li><b>Heuristic initialization on $Q$</b><br>\n",
    "For Q-learning, can you think of an initialization of $Q$ that would be better than plain zeros (for example by exploiting the maximum 1-step reward $r_{max}$)? One that, for instance, would drive the exploration towards unvisited states?\n",
    "<li><b>Reward shaping</b><br>\n",
    "Did you notice that falling into a hole brings no penalty? If we introduced a $-1$ reward for falling into a hole, would it change the optimal policy?\n",
    "<li><b>SARSA($\\lambda$)</b><br>\n",
    "SARSA is an on-policy method and we've seen that so is TD($\\lambda$) so it seems rather straightforward to implement a SARSA($\\lambda$) algorithm that, hopefully, will have better convergence properties than plain SARSA.\n",
    "<li><b>$Q(\\lambda)$</b><br>\n",
    "This time it is not as straightfoward to derive a $Q(\\lambda)$ algorithm from TD($\\lambda$), precisely because TD($\\lambda$) evaluates the policy being applied and not another one. Can you imagine a way to still perform $Q(\\lambda)$ updates? An answer is found in Watkins's thesis that introduces Q-learning in 1989. For a more recent approach and other references, see Sutton et al, <b>A new Q($\\lambda$) with interim forward view and Monte Carlo equivalence</b>, 2014)\n",
    "<li><b>SARSA and $Q$-learning with linear value function approximation</b><br>\n",
    "Can you implement an approximate version of SARSA and $Q$-learning with linear Q-function approximation $Q(s,a)=\\theta^T\\varphi(s,a)$?\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e512e-c48d-4c20-8a7a-e9ec99cb4746",
   "metadata": {},
   "source": [
    "# Fitted Q-Iteration\n",
    "\n",
    "TODO series of exercises to implement FQI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0dd8a9-d21e-4c1a-877c-57c524201df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

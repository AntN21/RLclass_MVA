{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6229bf",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/reinforcement-learning/\">https://supaerodatascience.github.io/reinforcement-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b7186",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Class 2: Policy and value optimization, formulating the problem</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d146450",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**  \n",
    "We move on to defining what is the optimization problem in RL and how we formulate it. Bt the end of this section, you should be able to:\n",
    "- link optimal policies and value functions,\n",
    "- give a definition for a policy optimization problem,\n",
    "- write the Bellman equations (evaluation and optimality) and define a value optimization problem,\n",
    "- sketch out methods for policy and value optimization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6daaab",
   "metadata": {},
   "source": [
    "## The optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d287c7",
   "metadata": {},
   "source": [
    "Recall that an optimal policy $\\pi = (\\pi_t)_{t\\in\\mathbb{N}}$ is a policy which dominates over any other policy in every state:\n",
    "$$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in S, \\ \\forall \\pi, \\ V^{\\pi^*}(s) \\geq V^\\pi(s)$$\n",
    "\n",
    "Recall also that for the discounted criterion over infinite horizon problems, there always exists at least one optimal policy under the form $\\pi:s\\mapsto a$.\n",
    "\n",
    "If all states have a non-zero probability of being visited by any policy, then an optimal policy should pick optimal actions along any trajectory starting in (any) $s_0$, so it should pick optimal actions in all states. Then finding a policy which maximize $V^\\pi$ in all states is actually the same as finding a policy which maximizes the expected value $V^\\pi(s_0)$ of a fixed initial state $s_0$, or the expected value over any distribution on initial states $\\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$.\n",
    "\n",
    "To fix ideas, let's write $J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$. Then:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**The policy optimization problem:**  \n",
    "An optimal policy is a solution to $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$.\n",
    "</div>\n",
    "\n",
    "Solving this core optimization problem is the problem we wish to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7a78a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is time to define state-action value functions, also called Q-function, as they are a central object in RL. A Q-function is a function $S\\times A \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "The state-action value function of policy $\\pi$ is noted $Q^\\pi$ and is the expected return of playing action $a$ in $s$, then playing policy $\\pi$ in any subsequent state.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>State-action value function of policy $\\pi$</b><br>\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(S_t, A_t, S_{t+1}\\right) \\bigg| S_0 = s, A_0=a, \\pi \\right)$$\n",
    "</div>\n",
    "\n",
    "To be precise and reuse the full notations from the MDP definition:\n",
    "\\begin{align*}\n",
    "Q^\\pi(s,a) &=\\mathbb{E}\\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a,\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1})\\end{array} \\right],\\\\\n",
    " &= \\mathbb{E}_{s'} \\left[ r(s,a,s') + \\gamma V^\\pi(s') \\right], \\\\\n",
    " &= r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ V^\\pi(s') \\right]\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img/Qfunctions.png\" style=\"height: 200px;\"></img>\n",
    "\n",
    "Note that this definition of Q-functions uses the $\\gamma$-discounted criterion, but that it can be straightforwardly extended to any other criterion.  \n",
    "Let $C((R_t)_{t\\in \\mathbb{N}})$ be a criterion defined on the sequence of reward random variables.  \n",
    "Then the return random variable $G^\\pi(s,a)$ is the random variable $C((R_t)_{t\\in \\mathbb{N}})$ given that $S_0 = s$, $A_0=a$, $A_t \\sim \\pi(S_t)$ for $t>0$, $S_{t+1}\\sim p(\\cdot|S_t,A_t)$, and $R_t = r(S_t,A_t,S_{t+1})$.  \n",
    "And the corresponding Q-function for this criterion is simply \n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left[ C((R_t)_{t\\in \\mathbb{N}}) \\quad \\Bigg| \\quadÂ \\begin{array}{l}S_0 = s, A_0=a,\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1})\\end{array} \\right|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a6a39",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Characterizing value functions: the Bellman equations (40 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74408b35",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Intuitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d279b9d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the maze below, where an agent can move North, South, East or West. The resulting transition is deterministic and a reward of $+1$ is gained when exiting the maze (which terminates the game). Otherwise all rewards are zero. Bumping into a wall terminates the game with a reward of zero.\n",
    "\n",
    "<img src=\"img/grid_raw.png\" width=\"200px\"></img>\n",
    "\n",
    "Let's consider the policy $\\pi$ that always moves East.\n",
    "\n",
    "<img src=\"img/grid_policy.png\" width=\"200px\"></img>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll**  \n",
    "[https://linkto.run/p/NO9LB7NP](https://linkto.run/p/NO9LB7NP)  \n",
    "Without writing any equation, what is the value of the top-right cell under this policy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798d071",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$V^\\pi((3,3)) = 1$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39556880",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's take $\\gamma=0.9$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll**  \n",
    "[https://linkto.run/p/03LL2V5H](https://linkto.run/p/03LL2V5H)  \n",
    "Without writing any equation, what is the value of the top-middle cell under this policy? What is the value of the bottom-right cell?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab6ac6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The value of $(2,3)$ is the expected discounted sum of what one gets from applying $\\pi$ from $(2,3)$. Since the $\\pi$ is deterministic and the transitions are deterministic too, $\\pi(2,3)$ always take us to state $(3,3)$. So $V^\\pi((2,3)) = 0 + \\gamma \\times V^\\pi((3,3)) = 0.9$.\n",
    "    \n",
    "The value of $(3,1)$ is the expected infinite sum of discounted rewards from $(3,1)$. Since the agent keeps bumping into the wall when applying $\\pi$, it never exits the maze and this is an infinite sum of zero terms. Hence $V^\\pi((2,3)) = 0$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184b4c9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's draw the value function.\n",
    "\n",
    "<img src=\"img/grid_vpi.png\" width=\"200px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c530970",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose you are currently in cell $(1,2)$ and would like to choose what action to take. Suppose also that you know the value function above. You need to put a scalar value on all four actions. To evaluate each action, let's estimate what we can get by applying the action and then using $\\gamma \\times V^\\pi(s)$ to estimate what can obtain in the long run after this first action. Define $Q^\\pi((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question / Poll**  \n",
    "What is $Q^\\pi((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we follow $\\pi$ after?  \n",
    "[https://linkto.run/p/5WV5GU62](https://linkto.run/p/5WV5GU62)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66093a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$Q^\\pi((1,2),N) = 0 + \\gamma \\cdot \\gamma^2 = 0.729$  \n",
    "$Q^\\pi((1,2),S) = 0 + \\gamma \\cdot 0 = 0$  \n",
    "$Q^\\pi((1,2),E) = 0 + \\gamma \\cdot 0 = 0$  \n",
    "$Q^\\pi((1,2),W) = 0$  \n",
    "The best action seems to be $N$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94867bb3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An optimal policy is quite easy to guess. Let's draw the optimal value function (the value function of any optimal policy).\n",
    "\n",
    "<img src=\"img/grid_vopt.png\" width=\"200px\"></img>\n",
    "\n",
    "Define $Q^*((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$ if it is followed by an optimal policy.\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question**   \n",
    "What is $Q^*((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we act optimally after? Rank the actions by utility.  \n",
    "https://linkto.run/p/3OG6IJO3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80935e7f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$Q^*$ is what we gain immediately, plus $\\gamma$ times what we expect to receive from applying an optimal policy in the state we reach by applying $a$.  \n",
    "$Q^*((1,2),N) = 0 + \\gamma\\times\\gamma^2=\\gamma^3$  \n",
    "$Q^*((1,2),S) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
    "$Q^*((1,2),E) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
    "$Q^*((1,2),W) = 0 + \\gamma\\times0=0$  \n",
    "The best action seems to be $N$, followed by $W$, after that $S$ and $E$ are tied.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce277c6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Question (no poll)</b><br>\n",
    "What property has the policy that always picks greedily the $Q^*$ maximizing action in each state?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4428d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "It is an optimal policy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f6732",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now suppose $(1,2)$ is a special slippery cell. Going North has a $0.7$ probability of actually reaching $(1,3)$, but also a $0.2$ probability of staying in $(1,2)$ and a $0.1$ probability of ending in $(2,2)$. Note that this changes the problem and the optimal expected return function $V^*$.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question (no poll)</b><br>\n",
    "Given this new problem, can you write $Q^*((1,2),N)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c07a14",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "When we take action $N$ in $(1,2)$, there are 3 possible outcomes:\n",
    "- with probability $0.7$, reach $(1,3)$ and get reward $0$,\n",
    "- with probability $0.2$, reach $(1,2)$ and get reward $0$,\n",
    "- with probability $0.1$, reach $(2,2)$ and get reward $0$.\n",
    "\n",
    "So what we can expect to get from applying $N$ in $(1,2)$ is:  \n",
    "\\begin{align*}\n",
    "    Q^*((1,2), N) &= 0.7 \\times (0+\\gamma V^*(1,3)) + 0.2\\times(0+\\gamma V^*(1,2)) + 0.1\\times(0+\\gamma V^*(2,2))\\\\\n",
    "    &= \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)\n",
    "\\end{align*}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0dac08",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you can remark that if we knew the action $\\pi^*((1,2))$ taken by an optimal policy in $(1,2)$, then $Q^*((1,2), \\pi^*(1,2))$ would actually be precisely the optimal long-term return $V^*$ (since it would be the expected return of a policy that acts optimally at every time step, including the first one).\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "Suppose an oracle tells us that $\\pi^*((1,2))=N$. Using the previous exercice, write $V^*(1,2)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd97a01",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "We have $V^*((1,2)) = Q^*((1,2),N)$, so\n",
    "$$V^*((1,2)) = \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f77581",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have introduced the key concepts upon which this secton is built: $V$ and $Q$ functions, and the relation between $V(s)$ and $V(s')$ when $s'$ can be reached from $s$ in one action. The next steps are now to write all this formally, prove strong properties and derive algorithms for computing value functions and policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c9924",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### The evaluation equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036527fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Drawing inspiration from the exercises above, we can define state-action value functions, also called Q-functions, which are a central object in RL. the state-action value function $Q^\\pi$.A Q-function is a function $S\\times A \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "The state-action value function of policy $\\pi$ is noted $Q^\\pi$ and is the expected return of playing action $a$ in $s$, then playing policy $\\pi$ in any subsequent state.\n",
    "\n",
    "<div class=\"alert alert-success\"><b>State-action value function</b><br>\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(S_t, A_t, S_{t+1}\\right) \\bigg| S_0 = s, A_0=a, \\pi \\right)$$\n",
    "</div>\n",
    "\n",
    "To be precise and reuse the full notations from the MDP definition:\n",
    "\\begin{align*}\n",
    "Q^\\pi(s,a) &=\\mathbb{E}\\left[ \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a,\\\\ A_t=\\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1})\\end{array} \\right],\\\\\n",
    " &= \\mathbb{E}_{s'} \\left[ r(s,a,s') + \\gamma V^\\pi(s') \\right], \\\\\n",
    " &= r(s,a) + \\gamma \\mathbb{E}_{s'} \\left[ V^\\pi(s') \\right]\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img/Qfunctions.png\" style=\"height: 200px;\"></img>\n",
    "\n",
    "Note that this definition of Q-functions uses the $\\gamma$-discounted criterion, but that it can be straightforwardly extended to any other criterion.  \n",
    "Let $C((R_t)_{t\\in \\mathbb{N}})$ be a criterion defined on the sequence of reward random variables.  \n",
    "Then the return random variable $G^\\pi(s,a)$ is the random variable $C((R_t)_{t\\in \\mathbb{N}})$ given that $S_0 = s$, $A_0=a$, $A_t \\sim \\pi(S_t)$ for $t>0$, $S_{t+1}\\sim p(\\cdot|S_t,A_t)$, and $R_t = r(S_t,A_t,S_{t+1})$.  \n",
    "And the corresponding Q-function for this criterion is simply \n",
    "$$Q^\\pi(s,a) = \\mathbb{E}\\left[ C((R_t)_{t\\in \\mathbb{N}}) \\quad \\Bigg| \\quadÂ \\begin{array}{l}S_0 = s, A_0=a,\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1})\\end{array} \\right|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d3e4f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's remark that $V^\\pi(s) = Q^\\pi(s,\\pi(s))$. Let's replace $a$ by $\\pi(s)$ above and we obtain an important equation to characterize $V^\\pi$.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"img/V-DP.png\" style=\"height: 200px;\"></img>\n",
    "$$V^\\pi(s) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V^\\pi(s') \\right]$$\n",
    "\n",
    "This equation uses $V^\\pi(s')$ in all $s'$ reachable from $s$ to define $V^\\pi(s)$.  \n",
    "Since this equation is true in all $s$, this provides as many equations as we have states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc559b0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Evaluation equation</b><br>\n",
    "$V^\\pi$ obeys the linear system of equations:\n",
    "$$\n",
    "V^\\pi\\left(s\\right) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V^\\pi(s') \\right]\\\\\n",
    "$$\n",
    "Similarly:\n",
    "$$\n",
    "Q^\\pi\\left(s,a\\right) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q^\\pi(s',\\pi(s')) \\right]\n",
    "$$\n",
    "</div>\n",
    "\n",
    "This leads to the introduction of the **Bellman evaluation operator**:\n",
    "<div class=\"alert alert-success\"><b>Evaluation operator $T^\\pi$</b><br>\n",
    "$T^\\pi$ is an operator on value functions, that transforms a function $V:S\\rightarrow \\mathbb{R}$ into:\n",
    "\\begin{align*}\n",
    "T^\\pi V\\left(s\\right) &= r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ V(s') \\right]\\\\\n",
    " &= r\\left(s,\\pi\\left(s\\right)\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,\\pi\\left(s\\right)\\right) V\\left(s'\\right)\n",
    "\\end{align*}\n",
    "    \n",
    "Similarly we can introduce an evaluation operator (with the same name $T^\\pi$) over state-action value functions. <br> \n",
    "$T^\\pi$ is an operator on state-action value functions, that transforms a function $Q:S\\times A\\rightarrow \\mathbb{R}$ into:\n",
    "\\begin{align*}\n",
    "T^\\pi Q\\left(s,a\\right) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ Q(s',\\pi(s')) \\right]\\\\\n",
    " &= r\\left(s,a\\right) + \\gamma \\sum\\limits_{s'\\in S} p\\left(s'|s,a\\right) Q\\left(s', \\pi\\left(s'\\right)\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "Note that, fundamentally, we have written 4 times the same thing in the block above.  \n",
    "So finding $V^\\pi$ (resp. $Q^\\pi$) boils down to solving the evaluation equation $V= T^\\pi V$ (resp. $Q = T^\\pi Q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31dd548",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have gone far from our original FrozenLake problem. Let's make all this very concrete:\n",
    "- A policy $\\pi$ is an agent's behaviour\n",
    "- In every state $s$, one can expect to gain $V^\\pi(s)$ in the long run by applying $\\pi$\n",
    "- $V^\\pi(s)$ is the sum of the reward on the first step $r(s,\\pi(s))$ and the expected long-term return from the next state $\\gamma \\mathbb{E}_{s'} \\left[V^\\pi(s')\\right]$ \n",
    "- The function $V^\\pi$ actually obeys the linear system of equations above that simply link the value of a state with the values of its successors in an episode.\n",
    "\n",
    "We can stop for a minute on the $T^\\pi$ evaluation operator (that maps a function $S\\rightarrow\\mathbb{R}$ to a function $S\\rightarrow\\mathbb{R}$) and the search for $V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af06d78",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Properties of $T^\\pi$</b><br>\n",
    "<ol>\n",
    "<li> $T^\\pi$ is an affine operator, it defines a linear system of equations.<br>\n",
    "<li> $T^\\pi$ is a contraction mapping<br>\n",
    "    Specifically, with $\\gamma<1$, $T^\\pi$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^\\pi$ (resp. $Q^\\pi$) is the unique solution to the (linear) fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^\\pi V$ (resp. $Q=T^\\pi Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3b535",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's use this second property to compute $Q^\\pi$ for the policy that always moves right on FrozenLake.\n",
    "\n",
    "Suppose we start with $Q_0(s,a) = 0$ for all $(s,a)$.\n",
    "\n",
    "Recall that, in FrozenLake, rewards are provided under the $r(s,a,s')$ form.\n",
    "\n",
    "Applying $T^\\pi$ once results in:\n",
    "$$Q_1(s,a) = \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma Q_0(s',\\pi(s')) \\right]$$\n",
    "\n",
    "In plain words, $Q_1$ is the one-step expected return under policy $\\pi$.\n",
    "\n",
    "Applying $T^\\pi$ twice results in:\n",
    "$$Q_2(s,a) = \\sum_{s'} p(s'|s,a) \\left[ r(s,a,s') + \\gamma Q_1(s',\\pi(s')) \\right]$$\n",
    "\n",
    "This is the two-step expected return.\n",
    "\n",
    "And so on.\n",
    "\n",
    "If we apply $T^\\pi$ enough times, $Q_n$ should become closer to $Q^\\pi$, whatever the chosen value for $Q_0$.\n",
    "\n",
    "In more formal words, because $T^\\pi$ is a contraction mapping, the sequence $Q_{n+1} = T^\\pi Q_n$ converges to $T^\\pi$'s fixed point.\n",
    "\n",
    "Let us live-code this.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Live coding</b><br>\n",
    "Let's compute the sequence $Q_{n+1} = T^\\pi Q_n$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764f7ed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pi = fl.RIGHT*np.ones((env.observation_space.n), dtype=np.uint8)\n",
    "nb_iter = 20\n",
    "gamma = 0.9\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Qpi_sequence = [Q]\n",
    "for i in range(nb_iter):\n",
    "    Qnew = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[x][a]\n",
    "            for o in outcomes:\n",
    "                p = o[0]\n",
    "                y = o[1]\n",
    "                r = o[2]\n",
    "                Qnew[x,a] += p * (r + gamma * Q[y,pi[y]])\n",
    "    Q = Qnew\n",
    "    Qpi_sequence.append(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b64904",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\"><b>Live coding</b><br>\n",
    "Let's plot the sequence of $\\| Q_n - Q_{n-1} \\|_\\infty$ to verify the convergence of the sequence.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b6801",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "residuals = []\n",
    "for i in range(1, len(Qpi_sequence)):\n",
    "    residuals.append(np.max(np.abs(Qpi_sequence[i]-Qpi_sequence[i-1])))\n",
    "\n",
    "plt.plot(residuals)\n",
    "plt.figure()\n",
    "plt.semilogy(residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098e642",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### The optimality equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a5012",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can unfold the same kind of reasoning on the value of an optimal policy. We write:\n",
    "$$V^{\\pi^*} = V^*, \\quad Q^{\\pi^*} = Q^*$$\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Theorem: Optimal greedy policy</b><br>\n",
    "Any policy $\\pi$ defined by $\\pi(s) \\in \\arg\\max\\limits_{a\\in A} Q^*(s,a)$ is an optimal policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5a9f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And $Q^*$ obeys the same type of recurrence relation:\n",
    "\n",
    "<div class=\"alert alert-success\"><b>Theorem: Bellman optimality equation</b><br>\n",
    "The optimal value function obeys:\n",
    "\\begin{align*}\n",
    "    V^*(s) &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V^*(s') \\right]\\\\\n",
    "        &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V^*(s') \\right]\n",
    "\\end{align*}\n",
    "or in terms of $Q$-functions:\n",
    "\\begin{align*}\n",
    "    Q^*(s,a) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q^*(s',a') \\right]\\\\\n",
    "        &= r(s,a) + \\gamma \\sum\\limits_{s'\\in S}p(s'|s,a) \\max\\limits_{a'\\in A} Q^*(s',a')\n",
    "\\end{align*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2d232",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As for the evaluation equation, we have actually written 4 times the same thing in the block above.  \n",
    "We have also defined the **Bellman optimality operator $T^*$** (on $V$ and $Q$ functions) as:\n",
    "<div class=\"alert alert-success\"><b>Bellman optimality operator</b><br>\n",
    "$$\\left(T^*V\\right)(s) = \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V(s') \\right]$$\n",
    "$$\\left(T^*Q\\right)(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q(s',a') \\right]$$\n",
    "</div>\n",
    "\n",
    "So finding $V^*$ (resp. $Q^*$) boils down to solving $V= T^* V$ (resp. $Q = T^* Q$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ad238",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-success\"><b>Properties of $T^*$</b><br>\n",
    "<ol>\n",
    "<li> $T^*$ is non-linear.<br>\n",
    "<li> $T^*$ is a contraction mapping<br>\n",
    "With $\\gamma<1$, $T^*$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
    "$\\Rightarrow$ With $\\gamma<1$, $V^*$ (resp. $Q^*$) is the unique solution to the fixed point equation:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^* V$ (resp. $Q=T^* Q$).\n",
    "</ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2c7eac-fbff-48e4-abd3-f0fbc8aee546",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887122ef-64e7-42f7-b83e-a684377dede3",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 5: Policy gradient algorithms</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf06b7-7e7f-4587-ae8d-656436cc4c15",
   "metadata": {},
   "source": [
    "In this chapter we first depart from the (approximate) dynamic programming framework we have worked with so far. We turn back to a criterion for optimality we introduced in the first chapters, namely the average value of a policy across (starting) states. We explore the question of writing the gradient of this criterion with respect to policy parameters, which leads us to the very important policy gradient theorem and the family of policy gradient algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d20f8-d89f-456e-980a-b1a9f2653a42",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**   \n",
    "By the end of this chapter, you should be able to:\n",
    "- recall and explain the policy gradient theorem\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ae3c4-5352-4a2c-b178-49f9d050d693",
   "metadata": {},
   "source": [
    "# Policy gradient methods\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Bottomline question:**   \n",
    "The previous chapters have focussed on *action-value methods*; they aimed at estimating $Q^*$ in order to deduce $\\pi^*$, or they jointly optimized $Q$ and $\\pi$. Could we directly optimize $\\pi$?\n",
    "</div>\n",
    "\n",
    "Suppose we have a policy $\\pi_\\theta$ parameterized by a vector $\\theta$. Our goal is to find the parameter $\\theta^*$ corresponding to $\\pi^*$.\n",
    "\n",
    "Remarks:\n",
    "- $\\pi_\\theta$ might not be able to represent $\\pi^*$. We will take a shortcut and call $\\pi^*$ the best policy among the $\\pi_\\theta$ ones.\n",
    "- For discrete state and action spaces, the tabular policy representation is a special case of policy parameterization.\n",
    "- Policy parameterization is a (possibly useful) way of introducing prior knowledge on the set of the desired policies.\n",
    "- The optimal deterministic policies might not belong to the policy subspace of $\\pi_\\theta$, thus it makes sense to consider stochastic policies for $\\pi_\\theta$.\n",
    "- It makes even more sense to consider stochastic policies that it opens the family of environments that we can tackle, like partially observable MDPs or multi-player games.\n",
    "\n",
    "For stochastic policies, we shall write $\\pi_\\theta(a|s)$.\n",
    "\n",
    "In the remainder of the chapter, we will assume that $\\pi_\\theta$ is differentiable with respect to $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98a37c-3be9-4820-bb3d-f9ba43eb6911",
   "metadata": {},
   "source": [
    "Suppose now we define some performance metric $J(\\pi_\\theta) = J(\\theta)$. If $J$ is differentiable and a stochastic estimate $\\tilde{\\nabla}_\\theta J(\\theta)$ of the gradient is available, then we can define the stochastic gradient ascent update procedure:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\tilde{\\nabla}_\\theta J(\\theta).$$\n",
    "\n",
    "We will call **policy gradient methods** all methods that follow such a procedure (whether or not they also learn a value function or not).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Polcy gradient method**   \n",
    "We call **policy gradient method** any method that performs stochastic gradient ascent on the policy's parameters.  \n",
    "Given a stochastic estimate $\\tilde{\\nabla}_\\theta J(\\theta)$ of a policy's performance criterion with respect to the policy's parameters, such a method implements the update procedure: \n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\tilde{\\nabla}_\\theta J(\\theta).$$\n",
    "</div>\n",
    "\n",
    "Remarks: \n",
    "- Note that $J$ is a generic criterion. For example, $J$ could be defined as the $\\gamma$-discounted value of a starting state (or a distribution of starting states), or as the undiscounted reward over a certain horizon, or as the average reward.\n",
    "- Note that this family of methods can use any gradient estimate for $\\tilde{\\nabla}_\\theta J(\\theta)$: formal calculus, finite differences, automated differentiation, evolution strategies, etc.\n",
    "- Why is it interesting to look at policy gradient methods? Because there is no maximization step ($\\max_a Q(s,a)$) during evaluation, which might be costly, especially for continuous actions, but only a call to $\\pi_\\theta(s)$ (or a draw from $\\pi_\\theta(a|s)$). This argument makes actor-critic architectures or direct policy search a method of choice for continuous actions domains (especially common in Robotics) and Policy Gradient is one of them.\n",
    "- When do policy gradient approaches outperform value-based ones? It's hard to give a precise criterion; it really depends on the problem. One thing that comes into play is how easy it is to approximate the optimal policy or the optimal value function. If one is simpler than the other (by \"simpler\", we mean \"it is easier to find a parameterization whose spanned function space almost includes the function to approximate\"), then it is a good heuristic to try to approximate it. But this criterion might itself be hard to assess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792df89-be43-455d-af32-2052d1bd8f15",
   "metadata": {},
   "source": [
    "**Notations**\n",
    "\n",
    "- We consider probability density functions $p(X)$ for all random variables $X$.\n",
    "- For a policy $\\pi_\\theta$ and a random variable $X$ we write indifferently $p(X|\\pi_\\theta) = p(X|\\theta)$.\n",
    "- A trajectory is noted $\\tau = (s_t,a_t)_{t\\in \\mathbb{N}}$.\n",
    "- The state random variable at step $t$ is $S_t$ and its law's density is $p_t(s)$.\n",
    "- The action random variable at step $t$ is $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96beae82-90be-4b77-85e5-21a43733629e",
   "metadata": {},
   "source": [
    "# The policy gradient theorem\n",
    "\n",
    "## Reminder on the policy optimization objective\n",
    "\n",
    "Recal the policy optimization objective defined in previous chapters, given a distribution $p_0$ on states:  \n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim p_0} \\left[ V^{\\pi} (s) \\right].$$\n",
    "Or equivalently:  \n",
    "$$J(\\pi) = \\mathbb{E}_{(s_i,a_i)_{i \\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)  | \\pi, p_0 \\right].$$\n",
    "Since $p_0$ is supposed known and fixed, we will omit it in the rest of this chapter for the sake of readability.  \n",
    "We can switch the sum and the expectation and get:  \n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{(s_i,a_i)_{i \\in \\mathbb{N}}} \\left[ r(s_t,a_t)  | \\pi \\right].$$\n",
    "But $\\mathbb{E}_{(s_i,a_i)_{i \\in \\mathbb{N}}} \\left[ r(s_t,a_t)  | \\pi \\right] = \\mathbb{E}_{s_t,a_t} \\left[ r(s_t,a_t)  | \\pi \\right]$. So:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{s_t,a_t} \\left[ r(s_t,a_t)  | \\pi \\right].$$\n",
    "Now let's introduce the density of $(s_t,a_t)$:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\int_S \\int_A r(s_t,a_t) p(s_t,a_t|\\pi) ds_t da_t.$$\n",
    "But $p(s_t,a_t|\\pi) = p(s_t|\\pi) p(a_t|s_t,\\pi)$. By definition, $p(s_t|\\pi) = p_t(s|\\pi)$ and $p(a_t=a|s_t=s,\\pi) = \\pi(a|s)$. So:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\int_S \\int_A r(s,a) p_t(s|\\pi) \\pi(a|s) ds da.$$\n",
    "Let us isolate the terms that concern only states:\n",
    "$$J(\\pi) = \\int_S \\left[ \\int_A r(s,a) \\pi(a|s) da \\right] \\sum_{t=0}^\\infty \\gamma^t p_t(s|\\pi) ds.$$\n",
    "Let's note $\\rho^\\pi(s) = \\sum_{t=0}^\\infty \\gamma^t p_t(s|\\pi)$. We have encountered this quantity in previous chapters and called it the *state occupancy measure under policy $\\pi$ and starting distribution $p_0$*. It is not a proper distribution per se (it sums to $\\frac{1}{1-\\gamma}$), and is sometimes also called the *improper state distribution under policy $\\pi$* or the *improper state visitation frequency under policy $\\pi$*. Then we have:\n",
    "$$J(\\pi) = \\int_S \\left[ \\int_A r(s,a) \\pi (a|s) da \\right] \\rho^\\pi(s) ds.$$\n",
    "And so finally, with a slight notation abuse because $\\rho^\\pi$ is not a probability distribution:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ r(s,a) \\right].$$\n",
    "\n",
    "In plain words, the value of a policy $\\pi$ is the average value of the rewards when states are sampled according to $\\rho^\\pi$ and actions are sampled according to $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c60fd-7697-4aee-9e2d-db8b90616e2c",
   "metadata": {},
   "source": [
    "## The policy gradient theorem\n",
    "\n",
    "The crucial problem of computing $\\nabla_\\theta J(\\theta)$ lies in the fact that when $\\theta$ changes, both $\\pi$ and $\\rho^\\pi$ change jointly. So there seems to be no straighforward way of evaluating this gradient. One could fall back to a *finite differences* approach to estimating this gradient, but this would require trying out a series of increments $\\Delta \\theta$ which quickly becomes impractical (because the increment size is hard to tune, especially in stochastic systems, and also because of the sample inefficiency of the approach).\n",
    "\n",
    "Remark:\n",
    "- Let's not discard finite difference methods too quickly. They have their merits and showed great successes through methods such as [PEGASUS (Ng and Jordan, 2000)](https://arxiv.org/abs/1301.3878). Also, having random $\\Delta \\theta$ drawn from a Gaussian distribution is essentially what Evolution Strategies do, and [Salimans et al. (2017)](https://arxiv.org/abs/1703.03864) or [Chrabaszcz et al. (2018)](https://arxiv.org/abs/1802.08842) illustrated how that could be a scalable method to obtain gradient estimates in RL. We won't cover these topics here and leave them as exercises.  \n",
    "\n",
    "The key result of this chapter is that one can express the gradient of $J(\\theta)$ as directly proportional to the value of $Q^\\pi$ and the gradient of $\\pi$:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy gradient theorem:**  \n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ Q^\\pi(s,a) \\nabla_\\theta \\log\\pi(a|s)\\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877d21c-a9bc-400a-8a00-7f68e8d62bc3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Why is it not really a problem that $\\rho^\\pi$ is not a proper probability density function?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13668513-99c6-4d43-bc64-8913b22e24ae",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "It is not really a problem because it is still a measure, proportional to a density function.  \n",
    "We are interested in ascent *directions*, not the exact gradient. The latter is proportional to the former. So as long as we can sample from a distribution whose mass is proportional to $\\rho^\\pi$, we obtain a correct direction estimate.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed3fb7-e06c-48f4-afbc-717c92935251",
   "metadata": {},
   "source": [
    "The proof of this result is simple but a bit tedious. We can however give the general intuition. Let's consider trajectories $\\tau = (s_0,a_0,r_0,...)$ drawn according to $\\pi$ from the starting state. Each of these trajectories has an overall payoff of $G(\\tau) = \\sum_t \\gamma^t r_t$, and is drawn with probability density $p(\\tau|\\theta)$. Then the objective function can be written:\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\mathbb{E}_\\tau \\left[ G(\\tau) | \\theta \\right]\\\\\n",
    " &= \\int G(\\tau) p(\\tau | \\theta) d\\tau\n",
    "\\end{align}\n",
    "\n",
    "So the objective function's gradient is:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta \\int G(\\tau) p(\\tau|\\theta) d\\tau,\\\\\n",
    " &= \\int G(\\tau) \\nabla_\\theta p(\\tau|\\theta) d\\tau,\\\\\n",
    " &= \\int G(\\tau) p(\\tau|\\theta) \\frac{\\nabla_\\theta p(\\tau|\\theta)}{p(\\tau|\\theta)} d\\tau,\\\\\n",
    " &= \\mathbb{E}_\\tau \\left[ G(\\tau) \\nabla_\\theta \\log p(\\tau|\\theta) \\right].\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992e3d5-bc45-4830-b54a-0463d38fae9d",
   "metadata": {},
   "source": [
    "We have used the fact that $\\nabla_\\theta p(\\tau|\\theta) = p(\\tau|\\theta) \\nabla_\\theta \\log p(\\tau|\\theta)$, sometimes known as the *nabla-log* trick.  \n",
    "Let us study the $\\nabla_\\theta \\log p(\\tau|\\theta)$ term along a series of remarks.\n",
    "\n",
    "**Remark 1: law of $s_{t+1},a_{t+1}$ given the policy and history.**  \n",
    "One has $p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) p(a_{t+1} | s_{t+1}, (s_i,a_i)_{i \\in [0,t]}, \\theta)$.  \n",
    "But the transition model is Markovian, so $p(s_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | s_t, a_t)$.  \n",
    "And the law of $a_{t+1}$ is given by the policy, so $p(a_{t+1} | s_{t+1}, (s_i,a_i)_{i \\in [0,t]}, \\theta) = \\pi_\\theta(a_{t+1}|s_{t+1})$.  \n",
    "Consequently:\n",
    "$$p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_{t+1}|s_{t+1}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd6aeb-d089-4a3d-9137-1bcc62fed008",
   "metadata": {},
   "source": [
    "**Remark 2: probability density of a trajectory.**  \n",
    "Recall that $p(\\tau|\\theta) = p((s_t,a_t)_{t\\in [0,\\infty]}|\\theta)$.  \n",
    "This joint probability can be decomposed into conditional probabilities: $$p(\\tau|\\theta) = p(s_0,a_0|\\theta) \\prod_{t=0}^\\infty p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta).$$ \n",
    "The previous remarks allows us to simplify to: $$p(\\tau|\\theta) = p(s_0,a_0|\\theta) \\prod_{t=0}^\\infty p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_{t+1}|s_{t+1}).$$ \n",
    "By expanding the first term into $p(s_0)\\pi_\\theta(a_0|s_0)$ and reordering the terms inside the product, we obtain:\n",
    "$$p(\\tau|\\theta) = p(s_0) \\prod_{t=0}^\\infty p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_t|s_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1ea75-547b-4115-ad79-7de88f5d4194",
   "metadata": {},
   "source": [
    "**Remark 3: grad-log-prob of a trajectory.**  \n",
    "Now let us consider the full $\\nabla_\\theta \\log p(\\tau|\\theta)$ term. The previous remarks tell us that  \n",
    "$$\\nabla_\\theta \\log p(\\tau|\\theta) = \\nabla_\\theta \\log p(s_0) + \\sum_{t=0}^\\infty \\left[ \\nabla_\\theta \\log p(s_{t+1} | s_t, a_t) + \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\right].$$\n",
    "But the initial state distribution and the transition model do not depend on $\\theta$, so this expression boils down to:\n",
    "$$\\nabla_\\theta \\log p(\\tau|\\theta) = \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t).$$\n",
    "\n",
    "And we will admit the step which leads to:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right].$$\n",
    "\n",
    "We don't quite have the policy gradient as stated above, but this intermediate result actually already provides us with a straightforward algorithm, so let's mark it:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy gradient**  \n",
    "Given trajectories $\\tau=(s_t,a_t)_{t\\in [0,\\infty]}$ drawn according to policy $\\pi$, and evaluated through a criterion $G(\\tau)$, \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right].$$\n",
    "</div>\n",
    "\n",
    "So we could play policy $\\pi_\\theta$ from $p_0$, and collect pairs of states and actions $(s,a)$. After each trajectory terminates, sum the rewards obtained to get $G(\\tau)$, then sum over all encountered states and actions the $G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a|s)$ terms. This yields an ascent direction: we can take a stochastic gradient ascent step, and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fd865-00d3-4fec-beee-419416af88d1",
   "metadata": {},
   "source": [
    "**Interpretation**  \n",
    "Let us consider a fixed $s$.  \n",
    "Then $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ is the answer to \"in what direction should I change $\\theta$ to increase the log probability of taking action $a$ in $s$?\".  \n",
    "The expression above tells us that in order to improve the value of $\\pi_\\theta$, we should change $\\theta$ in a direction that is a linear combination of all $\\nabla_\\theta \\pi_\\theta(a|s)$, giving more weight to action $a$ in state $s$ in proportion to the value $G(\\tau)$ of the trajectory they belong too.   \n",
    "In even simpler words: if a trajectory was better than another, its action probabilities should be reinforced (hence the name of the algorithm).\n",
    "\n",
    "The next section will focus on implementing this algorithm (almost), but for now let us finish deriving the policy gradient theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0190d84-c24c-4648-809d-717556ac60c5",
   "metadata": {},
   "source": [
    "**Remark 4: the expected grad-log-prob lemma.**  \n",
    "Let us take a step aside and consider the expectation of $\\nabla_\\theta \\log \\pi_\\theta(a|s)$.  \n",
    "Let's simplify the notation and consider a generic probability density function $\\pi_\\theta(x)$. Then:\n",
    "In a given $s$:\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{a\\sim \\pi_\\theta(s)} [ \\nabla_\\theta \\log \\pi_\\theta(x) ] &= \\int_A \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s) da\\\\\n",
    " &= \\int_A \\pi_\\theta(a|s) da\\\\\n",
    " &= \\nabla_\\theta \\int_A \\pi_\\theta(a|s) da\\\\\n",
    " &= \\nabla_\\theta 1\\\\\n",
    " &= 0\n",
    "\\end{align}\n",
    "So we get that: \n",
    "$$\\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right] = \\mathbb{E}_{s\\sim\\rho^\\pi} \\left[ \\mathbb{E}_{a\\sim \\pi} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right] \\right] = \\mathbb{E}_{s\\sim\\rho^\\pi} [0]=  0.$$\n",
    "Actually, we can generalize this immediately: we can multiply $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ by anything that does not depend on $a$ and obtain the same result:\n",
    "$$\\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ b(s)\\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right] \\right] = 0.$$\n",
    "This will come in handy in a future section about baselines in policy gradients and will serve as a basis for the exercise on [Generalized Advantage Estimation (Schulman et al., 2016)](https://arxiv.org/abs/1506.02438), but for now it will help us finish proving the policy gradient theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f927a-89bb-430c-ae15-d31386759126",
   "metadata": {},
   "source": [
    "**Obtaining the policy gradient theorem**  \n",
    "Recall that for a $\\gamma$ discounted criterion:\n",
    "$$G(\\tau) = \\sum_{t'=0}^\\infty \\gamma^{t'} r_{t'}.$$\n",
    "Let us consider a given time step $t$, at which a certain $(s,a)$ is encountered in $\\tau$. Then:\n",
    "$$G(\\tau) = \\sum_{t'=0}^{t-1} \\gamma^{t'} r_{t'} + \\sum_{t'=t}^\\infty \\gamma^{t'} r_{t'}.$$\n",
    "Then we have:\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right],\\\\\n",
    " &= \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\left[\\sum_{t'=0}^{t-1} \\gamma^{t'} r_{t'} + \\sum_{t'=t}^\\infty \\gamma^{t'} r_{t'} \\right] \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right].\n",
    "\\end{align*}\n",
    "Since $t$ is the time of occurence of $(s,a)$ in $\\tau$, the first sum $\\sum_{t'=0}^{t-1} \\gamma^{t'} r_{t'}$ does not depend on $a$. Consequently (thanks to the last remark):\n",
    "$$\\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\left[\\sum_{t'=0}^{t-1} \\gamma^{t'} r_{t'} \\right] \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right] = 0.$$\n",
    "\n",
    "In other words, any reward obtained before action $a$ is taken should not participate in reinforcing the probability of $a$ in $s$. This seems rather intuitive: an action should be reinforced only based on its consequences, not on rewards acquired before taking the action.\n",
    "\n",
    "So, since $Q^\\pi(s,a) = \\mathbb{E} [ \\sum_{t'=t}^\\infty \\gamma^{t'} r_{t'} | S_t=s, A_t=a, \\theta]$, we finally obtain the policy gradient:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ Q^\\pi(s,a) \\nabla_\\theta \\log\\pi(a|s)\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82ad47-a819-4361-b92c-80889cf17f30",
   "metadata": {},
   "source": [
    "**Interpretation**  \n",
    "Let us rephrase and refine the previous interpretation.  \n",
    "Consider a fixed $s$. Then (again) $\\nabla_\\theta \\log \\pi(a|s)$ is the answer to \"in what direction should I change $\\theta$ to increase the log probability of taking action $a$ in $s$?\".   \n",
    "The policy gradient theorem tells us that in order to improve the value of $\\pi_\\theta$, we should change $\\theta$ in a direction that is a linear combination of all $\\nabla_\\theta \\pi(a|s)$, giving more weight to actions that provide a large $Q^\\pi(s,a)$. \n",
    "\n",
    "We can push the interpretation of $\\nabla_\\theta \\log \\pi(a|s) = \\frac{\\nabla_\\theta \\pi(a|s)}{\\pi(a|s)}$ a bit further. $\\nabla_\\theta \\pi(a_t|s_t)$ is a vector in parameter space that points in the direction of greatest increase of $\\pi(a|s)$. The update will encourage taking a step in this direction if the action provided high return (through $Q(s,a)$), but moving in this direction will be moderated if the action is already picked frequently (through $\\pi(a|s)$) so that other actions have a chance also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b455538-de3b-4b9b-a4e5-1783f3dd7acd",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "Recall the policy gradient theorem:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy gradient**  \n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ Q^\\pi(s,a) \\nabla_\\theta \\log\\pi(a|s)\\right]$$\n",
    "</div>\n",
    "\n",
    "So, to compute the gradient, we can run the policy within the environment, and this will provide us with states distributed according to $\\rho^\\pi$ and actions distributed according to $\\pi$. The full trajectory of states-actions-rewards provides a Monte Carlo estimate $G_t$ of $Q^\\pi(s_t,a_t)$ from any state $s_t$ traversed by the trajectory. In turn, this allows to estimate $Q^\\pi(s_t,a_t) \\nabla_\\theta \\log \\pi(a_t|s_t)$ for any of these states. The sum over all states provides the gradient estimate.\n",
    "\n",
    "This algorithm, introduced by [Williams (1992)](https://link.springer.com/article/10.1007/BF00992696) is called REINFORCE. It requires a finite-length trajectory and its pseudo-code goes as follows.\n",
    "1. Initialize policy parameter $\\theta$\n",
    "2. Generate a trajectory by playing $\\pi$: $s_0,a_0,r_0,...s_{T}$\n",
    "3. For $t\\in [1, 2, … , T]$:\n",
    "    1. Estimate return $G_t$\n",
    "    2. Update policy parameter: $\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla_\\theta \\log \\pi(a_t|s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b69a0-10b7-44a1-8ae1-2f6bf2236170",
   "metadata": {},
   "source": [
    "Modern deep learning libraries don't take gradients as inputs. Instead, they use automated differentiation to compute them for a given expression, given a batch of samples. So we would like to build an objective function such that its gradient estimated on the minibatch composed of the last trajectory's samples $(s,a)$ corresponds to the expression above.\n",
    "\n",
    "So we want to define $\\ell(\\theta)$ such that $-\\nabla_\\theta \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\ell(\\theta,s,a) \\right]$ coincides with $\\nabla_\\theta J(\\theta)$. Note that, computationally, the $G_t$ terms are just coefficients in the gradient estimate. So, very simply, taking $\\ell(\\theta) = Q(s,a)\\cdot \\nabla_\\theta \\log \\pi(a|s)$ achieves this goal as long as $Q(s,a)$ is an unbiased estimate of $Q^\\pi(s,a)$. \n",
    "\n",
    "**Super important remark**  \n",
    "In supervised learning, it is a common practice to monitor the empirical risk along training. \n",
    "But $L(\\theta) = -\\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\ell(\\theta,s,a) \\right]$ is not comparable to the losses one usually encounters in supervised learning in any way!  \n",
    "In particular, $-L(\\theta) \\neq J(\\theta)$. So it makes absolutely no sense monitoring the value of this artificial \"loss\" function along training, and even less interpreting its value as some performance metric. It is *just* an expression whose gradient coincides with the (opposite of the) policy gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91550080-e2b1-44d0-817b-8b78580fa619",
   "metadata": {},
   "source": [
    "Let's implement REINFORCE for discrete action spaces.\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Declare a neural networks approximator for a categorical distribution for $\\pi(a|s)$.  \n",
    "The `forward` method should output action probabilities.  \n",
    "Define a `sample_action` method that draws from the actions probabilities in state $s$.  \n",
    "Define a `log_prob` method that returns the log probability of an $(s,a)$ pair.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f71429-6009-4ad3-842d-7ce51919a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class policyNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        n_action = env.action_space.n\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, n_action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_scores = self.fc2(x)\n",
    "        return F.softmax(action_scores,dim=1)\n",
    "\n",
    "    def sample_action(self, x):\n",
    "        probabilities = self.forward(x)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        return action_distribution.sample().item()\n",
    "\n",
    "    def log_prob(self, x, a):\n",
    "        probabilities = self.forward(x)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        return action_distribution.log_prob(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4dd14-c15b-4937-8c5f-90511d27b57e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Define a class that implements REINFORCE. Instead of drawing a single trajectory as in the pseudo-code above, include an option to draw several trajectories with the same policy. This will better reflect $\\rho^\\pi$ and provide less noisy gradients.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760f6951-aba0-42e5-9fd8-ff3bfd43a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "class reinforce_agent:\n",
    "    def __init__(self, config, policy_network):\n",
    "        self.device = \"cuda\" if next(policy_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(policy_network.parameters()).dtype\n",
    "        self.policy = policy_network\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.99\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()),lr=lr)\n",
    "        self.nb_episodes = config['nb_episodes'] if 'nb_episodes' in config.keys() else 1\n",
    "    \n",
    "    def one_gradient_step(self, env):\n",
    "        # run trajectories until done\n",
    "        episodes_sum_of_rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        returns = []\n",
    "        for ep in range(self.nb_episodes):\n",
    "            x,_ = env.reset()\n",
    "            rewards = []\n",
    "            episode_cum_reward = 0\n",
    "            while(True):\n",
    "                a = self.policy.sample_action(torch.as_tensor(x))\n",
    "                y,r,d,_,_ = env.step(a)\n",
    "                states.append(x)\n",
    "                actions.append(a)\n",
    "                rewards.append(r)\n",
    "                episode_cum_reward += r\n",
    "                x=y\n",
    "                if d:\n",
    "                    # compute returns-to-go\n",
    "                    new_returns = []\n",
    "                    G_t = 0\n",
    "                    for r in reversed(rewards):\n",
    "                        G_t = r + self.gamma * G_t\n",
    "                        new_returns.append(G_t)\n",
    "                    new_returns = list(reversed(new_returns))\n",
    "                    returns.extend(new_returns)\n",
    "                    episodes_sum_of_rewards.append(episode_cum_reward)\n",
    "                    break\n",
    "        # make loss\n",
    "        returns = torch.tensor(returns)\n",
    "        log_prob = self.policy.log_prob(torch.as_tensor(np.array(states)),torch.as_tensor(np.array(actions)))\n",
    "        loss = -(returns * log_prob).mean()\n",
    "        # gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return np.mean(episodes_sum_of_rewards)\n",
    "\n",
    "    def train(self, env, nb_rollouts):\n",
    "        avg_sum_rewards = []\n",
    "        for ep in trange(nb_rollouts):\n",
    "            avg_sum_rewards.append(self.one_gradient_step(env))\n",
    "        return avg_sum_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7c7a8-3cd7-4294-aaeb-62c0f42e248f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Run your agent for 50 gradient steps on `CartPole-v1`, drawing 10 episodes at each step.  \n",
    "Experiment with another number of episodes and vary the gradient step size.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a0c209-97bd-49fd-9dad-c08d6abb3a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:34<00:00,  1.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf6c2f9f70>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "config = {'gamma': .99,\n",
    "          'learning_rate': 0.01,\n",
    "          'nb_episodes': 10\n",
    "         }\n",
    "\n",
    "pi = policyNetwork(env)\n",
    "agent = reinforce_agent(config, pi)\n",
    "returns = agent.train(env,50)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0827e2da-5f69-4723-81ab-c2be40317fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/emmanuel/git_repos/RLclass_MVA/notebooks/videos/reinforce_policy-episode-0.mp4.\n",
      "Moviepy - Writing video /home/emmanuel/git_repos/RLclass_MVA/notebooks/videos/reinforce_policy-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/emmanuel/git_repos/RLclass_MVA/notebooks/videos/reinforce_policy-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "#test_env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array_list\")\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "s,_ = test_env.reset()\n",
    "with torch.no_grad():\n",
    "    for t in range(1000):\n",
    "        a = pi.sample_action(torch.as_tensor(s))\n",
    "        s2,r,d,trunc,_ = test_env.step(a)\n",
    "        s = s2\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "save_video(test_env.render(), \"videos\", fps=test_env.metadata[\"render_fps\"], name_prefix=\"reinforce_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c1aed4-e7b1-45e8-af8f-19bd26157b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/reinforce_policy-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/reinforce_policy-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91544b34-5c06-45d5-9424-46b4916afafc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "The code above is slightly inefficient as it passes each state through the network twice: one when drawing the action (`sample_action`), the other when computing log probabilities (`log_prob`). Make this a bit more efficient by writing a `sample_action_and_log_prob` function in the `reinforce_agent` class, which jointly draws the action and computes its log probability, enabling storing the log probabilities along training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c7566-63cf-47f6-b94e-4939b5fcb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/no_solution_yet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d797ec8-ab32-4779-86de-714055e0befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "class reinforce_agent:\n",
    "    def __init__(self, config, policy_network):\n",
    "        self.device = \"cuda\" if next(policy_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(policy_network.parameters()).dtype\n",
    "        self.policy = policy_network\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.99\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()),lr=lr)\n",
    "        self.nb_episodes = config['nb_episodes'] if 'nb_episodes' in config.keys() else 1\n",
    "\n",
    "    def sample_action_and_log_prob(self, x):\n",
    "        probabilities = self.policy(torch.as_tensor(x))\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        action = action_distribution.sample()\n",
    "        log_prob = action_distribution.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "    \n",
    "    def one_gradient_step(self, env):\n",
    "        # run trajectories until done\n",
    "        episodes_sum_of_rewards = []\n",
    "        log_probs = []\n",
    "        returns = []\n",
    "        for ep in range(self.nb_episodes):\n",
    "            x,_ = env.reset()\n",
    "            rewards = []\n",
    "            episode_cum_reward = 0\n",
    "            while(True):\n",
    "                a, log_prob = self.sample_action_and_log_prob(x)\n",
    "                y,r,d,_,_ = env.step(a)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(r)\n",
    "                episode_cum_reward += r\n",
    "                x=y\n",
    "                if d:\n",
    "                    # compute returns-to-go\n",
    "                    new_returns = []\n",
    "                    G_t = 0\n",
    "                    for r in reversed(rewards):\n",
    "                        G_t = r + self.gamma * G_t\n",
    "                        new_returns.append(G_t)\n",
    "                    new_returns = list(reversed(new_returns))\n",
    "                    returns.extend(new_returns)\n",
    "                    episodes_sum_of_rewards.append(episode_cum_reward)\n",
    "                    break\n",
    "        # make loss\n",
    "        returns = torch.tensor(returns)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        loss = -(returns * log_probs).mean()\n",
    "        # gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return np.mean(episodes_sum_of_rewards)\n",
    "\n",
    "    def train(self, env, nb_rollouts):\n",
    "        avg_sum_rewards = []\n",
    "        for ep in trange(nb_rollouts):\n",
    "            avg_sum_rewards.append(self.one_gradient_step(env))\n",
    "        return avg_sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3684a5e-5e6e-4132-9267-9745c2e46a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 21.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf640b94f0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "config = {'gamma': .99,\n",
    "          'learning_rate': 0.01,\n",
    "          'nb_episodes': 10\n",
    "         }\n",
    "\n",
    "pi = policyNetwork(env)\n",
    "agent = reinforce_agent(config, pi)\n",
    "returns = agent.train(env,50)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1752077-e1c9-4c75-b122-0d571057ca31",
   "metadata": {},
   "source": [
    "One key feature of policy gradient algorithms is that they are on-policy: they require the data to have been collected by the current policy, and discard this data once a gradient step is taken. Although this might seem sample inefficient, it can turn out to be an acceptable compromise if the policy gradient steps take the policy towards good returns quickly enough.\n",
    "\n",
    "A direct consequence of this on-policy property, is that policy gradients don't use experience replay buffers. Instead, they can take advantage of parallel computation to collect samples. We saw that running several rollouts with the same policy greatly helped in reducing the gradient's variance.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Gymnasium provides a [vectorized environments](https://gymnasium.farama.org/api/vector/) class, which enables running multiple independent copies of the same environment in parallel. Modify the previous code to use these environments.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033998fa-de4c-4a1a-965c-73e69f9c37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "## Make vector env\n",
    "#envs = gym.vector.make(\"CartPole-v1\", num_envs=3)\n",
    "envs = gym.make_vec(\"CartPole-v1\", num_envs=10)\n",
    "## Reset all environments\n",
    "states, _ = envs.reset(seed=42)\n",
    "#print(states)\n",
    "## Choose actions\n",
    "actions = envs.action_space.sample()\n",
    "#print(actions)\n",
    "## Step\n",
    "observations, rewards, termination, truncation, infos = envs.step(actions)\n",
    "#print(observations)\n",
    "#print(infos)\n",
    "\n",
    "for t in range(50):\n",
    "    actions = envs.action_space.sample()\n",
    "    observations, rewards, termination, truncation, infos = envs.step(actions)\n",
    "    if not infos=={}:\n",
    "        print(\"step\", t)\n",
    "        print(\"observations\\n\", observations)\n",
    "        print(\"termination\", termination)\n",
    "        print(\"truncation\", truncation)\n",
    "        print(\"infos\\n\", infos)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb015050-8c21-4bb6-81b0-91a9fb0e4869",
   "metadata": {},
   "source": [
    "# Actor-critic algorithms and baselines in policy gradients\n",
    "\n",
    "**Actor-critic algorithms**  \n",
    "Suppose now that we don't want a Monte Carlo estimate of $Q^\\pi(s,a)$ in the Policy Gradient theorem, and are rather willing to store a function approximator $Q_w$ for $Q^\\pi(s,a)$. This leads us to store both a policy $\\pi_\\theta$ and a value function $Q_w$. The value function *criticizes* the policy's selected actions by assigning numerical values to them, hence the names of *critic* and *actor*. So actor-critic algorithms are policy gradient algorithms that use an actor-critic architecture (remember: the opposite is not necessarily true).\n",
    "\n",
    "The $Q_w$ function of the critic can be learned as a risk minimization problem (eg. a least squares fitting problem), using temporal differences. Note that although this learning is off-policy, the data collection for the application of the policy gradient theorem still requires on-policy data.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Write the pseudo code of a policy gradient algorithm which estimates a parametric approximator $Q_w$ of $Q^\\pi$ using TD(0).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1742f2b-771e-480b-8742-cfb52456f8df",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "TODO\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc7ed3-aa5c-46f3-9119-5b3f0a1f853a",
   "metadata": {},
   "source": [
    "**Baslines in policy gradients**  \n",
    "Using estimators of $Q^\\pi$ in the application of the policy gradient theorem yields gradient estimates which often have a high variance. Following the expected grad-log-prob lemma, a common practice consists in substracting an action-independent *baseline* $b(s)$ from the estimate of $Q^\\pi(s,a)$, yielding:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\left( Q^\\pi(s,a) -b(s) \\right) \\nabla_\\theta \\log\\pi(a|s)\\right].$$\n",
    "\n",
    "It is rather easy to remark that this baseline does not affect the gradient estimate since it's expected value is zero (thanks to the expected grad-log-prob lemma). However it can contribute to strongly decrease the estimate's variance. One common choice for such a baseline is the policy's value function $V^\\pi$. This introduces an *advantage* estimation problem within policy gradient algorithms, where the advantage is the function defined as:\n",
    "$$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s).$$\n",
    "\n",
    "Remark that the temporal difference at each time step $\\delta = r + \\gamma V^\\pi(s') - V^\\pi(s)$ is an estimate of the advantage $A^\\pi(s,a)$. Using this remark, a simple one-step Actor-Critic method based on TD(0) and a value function $V_w$ goes as follows:\n",
    "1. In $s$, draw $a \\sim \\pi$\n",
    "2. Observe $r, s'$\n",
    "3. Compute $\\delta = r + \\gamma V_w(s') - V_w(s)$\n",
    "4. Update critic's parameters (TD(0) step) $w \\leftarrow w + \\alpha \\delta \\nabla_w V_w(s)$\n",
    "5. Update actor's parameters (policy gradient theorem) $\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta \\log \\pi(a|s)$\n",
    "6. $s\\leftarrow s'$ and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054b547-837e-4e7a-9f27-8849fcca4f71",
   "metadata": {},
   "source": [
    "# A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8712de-db1a-43a6-9d5c-c90d1741b5fb",
   "metadata": {},
   "source": [
    "# TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c7b87-6d9a-4a2b-ac8e-1c942d251978",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06d3e2-608b-4823-b6b2-a4238a7e8c5d",
   "metadata": {},
   "source": [
    "## REINFORCE on continuous action domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a91884-4238-4c04-87fb-fc33d6c6040e",
   "metadata": {},
   "source": [
    "## Policy gradients for the finite horizon criterion\n",
    "\n",
    "Derive the PG for the total reward, finite horizon criterion.\n",
    "\n",
    "Implement a REINFORCE algorithm (maybe more? an AC algorithm?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b11d3b9-3790-4df0-8ec5-7ea7ed265428",
   "metadata": {},
   "source": [
    "## Generalized advantage estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18138b35-51dc-4f17-98aa-8db737a218d2",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d725ead-ea20-495e-95de-a03c771ce2f9",
   "metadata": {},
   "source": [
    "## Gradient-free policy search\n",
    "\n",
    "OpenAI ES and Canonical ES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}













import gymnasium as gym
import gym.envs.toy_text.frozen_lake as fl
# use render_mode="human" to open the game window
env = gym.make('FrozenLake-v1', render_mode="ansi")
env.reset()
print(env.render())


## Run this only if you have used render_mode="human" in the cell above
#env.close()














print(env.observation_space)
print(env.action_space)





actions = {fl.LEFT: '\u2190', fl.DOWN: '\u2193', fl.RIGHT: '\u2192', fl.UP: '\u2191'}

def to_s(row,col):
    return row*env.unwrapped.ncol+col

def to_row_col(s):
    col = s%env.unwrapped.ncol
    row = int((s-col)/env.unwrapped.ncol)
    return row,col

print(actions)
row=3
col=2
a=2
print("Apply ", actions[2], " from (", row, ", ", col, "):", sep='')
for tr in env.unwrapped.P[to_s(row,col)][a]:
    print("  Reach (", to_row_col(tr[1]), ") and get reward ", tr[2], " with proba ", tr[0], ".", sep='')











import numpy as np
nb_episodes = 50000
horizon = 200

Vright = np.zeros(nb_episodes)
for i in range(nb_episodes):
    env.reset()
    for t in range(horizon):
        next_state, r, done, trunc, _, _ = env.step(fl.RIGHT)
        Vright[i] += r
        if done:
            break

Vleft  = np.zeros(nb_episodes)
for i in range(nb_episodes):
    env.reset()
    for t in range(horizon):
        next_state, r, done, trunc, _, _ = env.step(fl.LEFT)
        Vleft[i] += r
        if done:
            break

print("est. value of 'right' policy:", np.mean(Vright), "variance:", np.std(Vright))
print("est. value of 'left'  policy:", np.mean(Vleft),  "variance:", np.std(Vleft))





















































### WRITE YOUR CODE HERE
# If you get stuck, uncomment the line below to load a correction in this cell (then you can execute this code).


# %load solutions/RL1_exercise1.py













































